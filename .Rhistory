# calc ohc for min/max temps for each day to calc sdx for dnorm
minTag <- approx(pdt.i$Depth, pdt.i$MinTemp, xout = depth)
minTag <- minTag$y - isotherm
minT.ohc <- cp * rho * sum(minTag, na.rm = T) / 10000
maxTag <- approx(pdt.i$Depth, pdt.i$maxTemp, xout = depth)
maxTag <- maxTag$y - isotherm
maxT.ohc <- cp * rho * sum(maxTag, na.rm = T) / 10000
sdx <- sd(c(minT.ohc, maxT.ohc))
print(sdx)
# Perform hycom integration
dat[dat<isotherm] <- NA
dat <- dat - isotherm
ohc <- cp * rho * apply(dat, 1:2, sum, na.rm = T) / 10000
# compare hycom to that day's tag-based ohc
lik <- dnorm(ohc, tag.ohc, sdx)
if(i == 1){
# result will be array of likelihood surfaces
L.ohc <- array(0, dim = c(dim(lik), length(dateVec)))
}
idx <- which(dateVec == as.Date(time))
L.ohc[,,idx] = lik
print(paste(time, ' finished.', sep=''))
}
crs <- "+proj=longlat +datum=WGS84 +ellps=WGS84"
list.ohc <- list(x = lon-360, y = lat, z = L.ohc)
ex <- extent(list.ohc)
L.ohc <- brick(list.ohc$z, xmn=ex[1], xmx=ex[2], ymn=ex[3], ymx=ex[4], transpose=T, crs)
L.ohc <- flip(L.ohc, direction = 'y')
s <- stack(L.ohc)
return(s)
}
L.ohc <- calc.ohc(tagdata=pdt, g = g, ohc.dir = ohc.dir, dateVec=dateVec, isotherm='', raster = 'stack')
udates
i=17
# constants for OHC calc
cp <- 3.993 # kJ/kg*C <- heat capacity of seawater
rho <- 1025 # kg/m3 <- assumed density of seawater
# calculate midpoint of tag-based min/max temps
pdt$MidTemp <- (pdt$MaxTemp + pdt$MinTemp) / 2
# get unique time points
udates <- unique(pdt$Date)
iso.def <- FALSE
time <- udates[i]
pdt.i <- pdt[which(pdt$Date == time),]
pdt.i
nc <- open.ncdf(paste(ohc.dir, 'Lyd_', as.Date(time), '.nc', sep=''))
dat <- get.var.ncdf(nc, 'water_temp')
depth <- get.var.ncdf(nc, 'depth')
lon <- get.var.ncdf(nc, 'lon')
lat <- get.var.ncdf(nc, 'lat')
if(iso.def == FALSE) isotherm <- min(pdt.i$MinTemp, na.rm = T)
isotherm
tag <- approx(pdt.i$Depth, pdt.i$MidTemp, xout = depth)
tag <- tag$y - isotherm
tag.ohc <- cp * rho * sum(tag, na.rm = T) / 10000
tag
tag <- approx(pdt.i$Depth, pdt.i$MidTemp, xout = depth)
tag
tag.ohc
minTag <- approx(pdt.i$Depth, pdt.i$MinTemp, xout = depth)
minTag
cbind(minTag$x,minTag$y)
minTag <- minTag$y - isotherm
minT.ohc <- cp * rho * sum(minTag, na.rm = T) / 10000
maxTag <- approx(pdt.i$Depth, pdt.i$maxTemp, xout = depth)
maxTag <- maxTag$y - isotherm
maxT.ohc <- cp * rho * sum(maxTag, na.rm = T) / 10000
minT.ohc
tag.ohc
maxT.ohc
maxTag <- approx(pdt.i$Depth, pdt.i$maxTemp, xout = depth)
maxTag
pdt.i
maxTag <- approx(pdt.i$Depth, pdt.i$MaxTemp, xout = depth)
maxTag
calc.ohc <- function(tagdata, isotherm = '', ohc.dir, g, dateVec, raster = 'stack'){
# compare tag data to ohc map and calculate likelihoods
#' @param: tagdata is variable containing tag-collected PDT data
#' @param: isotherm is default '' in which isotherm is calculated
#' on the fly based on daily tag data. Otherwise, numeric isotherm
#' constraint can be specified (e.g. 20).
#' @param: ohc.dir is local directory where get.hycom downloads are
#' stored.
#' @return: likelihood is array of likelihood surfaces representing
#' matches between tag-based ohc and hycom ohc maps
# constants for OHC calc
cp <- 3.993 # kJ/kg*C <- heat capacity of seawater
rho <- 1025 # kg/m3 <- assumed density of seawater
# calculate midpoint of tag-based min/max temps
pdt$MidTemp <- (pdt$MaxTemp + pdt$MinTemp) / 2
# get unique time points
udates <- unique(pdt$Date)
if(isotherm != '') iso.def <- TRUE else iso.def <- FALSE
for(i in 1:length(udates)){
time <- udates[i]
pdt.i <- pdt[which(pdt$Date == time),]
# open day's hycom data
nc <- open.ncdf(paste(ohc.dir, 'Lyd_', as.Date(time), '.nc', sep=''))
dat <- get.var.ncdf(nc, 'water_temp')
depth <- get.var.ncdf(nc, 'depth')
lon <- get.var.ncdf(nc, 'lon')
lat <- get.var.ncdf(nc, 'lat')
# isotherm is minimum temperature recorded for that time point
if(iso.def == FALSE) isotherm <- min(pdt.i$MinTemp, na.rm = T)
# perform tag data integration
tag <- approx(pdt.i$Depth, pdt.i$MidTemp, xout = depth)
tag <- tag$y - isotherm
tag.ohc <- cp * rho * sum(tag, na.rm = T) / 10000
# calc ohc for min/max temps for each day to calc sdx for dnorm
minTag <- approx(pdt.i$Depth, pdt.i$MinTemp, xout = depth)
minTag <- minTag$y - isotherm
minT.ohc <- cp * rho * sum(minTag, na.rm = T) / 10000
maxTag <- approx(pdt.i$Depth, pdt.i$MaxTemp, xout = depth)
maxTag <- maxTag$y - isotherm
maxT.ohc <- cp * rho * sum(maxTag, na.rm = T) / 10000
sdx <- sd(c(minT.ohc, maxT.ohc))
print(sdx)
# Perform hycom integration
dat[dat<isotherm] <- NA
dat <- dat - isotherm
ohc <- cp * rho * apply(dat, 1:2, sum, na.rm = T) / 10000
# compare hycom to that day's tag-based ohc
lik <- dnorm(ohc, tag.ohc, sdx)
if(i == 1){
# result will be array of likelihood surfaces
L.ohc <- array(0, dim = c(dim(lik), length(dateVec)))
}
idx <- which(dateVec == as.Date(time))
L.ohc[,,idx] = lik
print(paste(time, ' finished.', sep=''))
}
crs <- "+proj=longlat +datum=WGS84 +ellps=WGS84"
list.ohc <- list(x = lon-360, y = lat, z = L.ohc)
ex <- extent(list.ohc)
L.ohc <- brick(list.ohc$z, xmn=ex[1], xmx=ex[2], ymn=ex[3], ymx=ex[4], transpose=T, crs)
L.ohc <- flip(L.ohc, direction = 'y')
s <- stack(L.ohc)
return(s)
}
L.ohc <- calc.ohc(tagdata=pdt, g = g, ohc.dir = ohc.dir, dateVec=dateVec, isotherm='', raster = 'stack')
expts = data.frame(
start=c(as.Date('2008-09-19')),
end=c(as.Date('2009-05-06')),
url=c('http://ncss.hycom.org/thredds/ncss/GLBu0.08/expt_90.6?'))
expts
L.ohc
plot(L.ohc[[1]])
plot(L.ohc[[2]])
expts = data.frame(
start=c(as.Date('1978-01-01')),
end=c(Sys.Date() + 1),
url=c('http://ncss.hycom.org/thredds/ncss/GLBu0.08/expt_90.6?'))
time
time[1] < expts$start[1]
time < expts$start[1]
expts$start[1]
time[1] > expts$end[nrow(expts)]
expts$end[nrow(expts)]
time
str(time)
as.Date(time)
time <- as.Date(time)
time[1] > expts$end[nrow(expts)]
(time[1] >= expts$start[i]) & (time[1] <= expts$end[i])
i=1
(time[1] >= expts$start[i]) & (time[1] <= expts$end[i])
url = expts$url[i]
url
expts = data.frame(
start=c(as.Date('1978-01-01')),
end=c(Sys.Date() + 1),
url=c('http://www.ncdc.noaa.gov/thredds/ncss/grid/OISST-V2-AVHRR-AMSR_agg/'))
url = expts$url[i]
url
var='sst'
url = sprintf('%svar=%s&', url, var)
lon
lat
lon = c(-90, -40)
lat = c(10, 55)
url
url = sprintf('%snorth=%f&west=%f&east=%f&south=%f&horizStride=1&',
url, lat[2], lon[1], lon[2], lat[1])
url = sprintf('%stime_start=%s%%3A00%%3A00Z&time_end=%s%%3A00%%3A00Z&timeStride=1&',
url, strftime(time[1], '%Y-%m-%dT00'),
strftime(time[1], '%Y-%m-%dT00'))
url
url = sprintf('%saddLatLon=true&', url)
url = sprintf('%sdisableProjSubset=on&vertCoord=&accept=netcdf', url)
url
sd(c(5.09+3.83))
sd(c(5.09,3.83))
expts = data.frame(
start=c(as.Date('1981-09-01')),
end=c(Sys.Date() + 1),
url=c('http://coastwatch.pfeg.noaa.gov/erddap/griddap/jplL4AvhrrOIv1fv2.htmlTable?analysed_sst'))
expts
'http://coastwatch.pfeg.noaa.gov/erddap/griddap/jplL4AvhrrOIv1fv2.htmlTable?analysed_sst[(2015-04-01T00:00:00Z):1:(2015-04-01T00:00:00Z)][(60):1:(10)][(-90):1:(-30)]'
?sprintf
time
sprintf('%stime_start=%s%%3A00%%3A00Z&time_end=%s%%3A00%%3A00Z&timeStride=1&',
url, strftime(time[1], '%Y-%m-%dT00'),
strftime(time[1], '%Y-%m-%dT00'))
url
url=c('http://coastwatch.pfeg.noaa.gov/erddap/griddap/jplL4AvhrrOIv1fv2.nc?analysed_sst'))
url=c('http://coastwatch.pfeg.noaa.gov/erddap/griddap/jplL4AvhrrOIv1fv2.nc?analysed_sst')
sprintf('%stime_start=%s%%3A00%%3A00Z&time_end=%s%%3A00%%3A00Z&timeStride=1&',
url, strftime(time[1], '%Y-%m-%dT00'),
strftime(time[1], '%Y-%m-%dT00'))
sprintf('%s[(%s%%T00:00:00Z):1:(%s%%T00:00:00Z)]',
url, strftime(time[1], '%Y-%m-%dT00'),
strftime(time[1], '%Y-%m-%dT00'))
'http://coastwatch.pfeg.noaa.gov/erddap/griddap/jplL4AvhrrOIv1fv2.htmlTable?analysed_sst[(2015-04-01T00:00:00Z):1:(2015-04-01T00:00:00Z)][(60):1:(10)][(-90):1:(-30)]'
sprintf('%s[(%s%%T00:00:00Z):1:(%s%%T00:00:00Z)]',
url, strftime(time[1], '%Y-%m-%dT00'),
strftime(time[1], '%Y-%m-%dT00'))
'http://coastwatch.pfeg.noaa.gov/erddap/griddap/jplL4AvhrrOIv1fv2.nc?analysed_sst[(2015-04-01T00:00:00Z):1:(2015-04-01T00:00:00Z)][(60):1:(10)][(-90):1:(-30)]'
sprintf('%s[(%s%T00:00:00Z):1:(%s%T00:00:00Z)]',
url, strftime(time[1], '%Y-%m-%dT00'),
strftime(time[1], '%Y-%m-%dT00'))
sprintf('%s[(%s%%T00:00:00Z):1:(%s%%T00:00:00Z)]',
url, strftime(time[1], '%Y-%m-%d'),
strftime(time[1], '%Y-%m-%d'))
sprintf('%s[(%s%%:00:00Z):1:(%s%%:00:00Z)]',
url, strftime(time[1], '%Y-%m-%dT00'),
strftime(time[1], '%Y-%m-%dT00'))
sprintf('%s[(%s%:00:00Z):1:(%s%%:00:00Z)]',
url, strftime(time[1], '%Y-%m-%dT00'),
strftime(time[1], '%Y-%m-%dT00'))
sprintf('%s[(%s%s%:00:00Z):1:(%s%%:00:00Z)]',
url, strftime(time[1], '%Y-%m-%dT00'),
strftime(time[1], '%Y-%m-%dT00'))
sprintf('%s[(%s%s:00:00Z):1:(%s%%:00:00Z)]',
url, strftime(time[1], '%Y-%m-%dT00'),
strftime(time[1], '%Y-%m-%dT00'))
sprintf('%s[(%s:00:00Z):1:(%s%%:00:00Z)]',
url, strftime(time[1], '%Y-%m-%dT00'),
strftime(time[1], '%Y-%m-%dT00'))
'http://coastwatch.pfeg.noaa.gov/erddap/griddap/jplL4AvhrrOIv1fv2.nc?analysed_sst[(2015-04-01T00:00:00Z):1:(2015-04-01T00:00:00Z)][(60):1:(10)][(-90):1:(-30)]'
sprintf('%s[(%s:00:00Z):1:(%s:00:00Z)]',
url, strftime(time[1], '%Y-%m-%dT00'),
strftime(time[1], '%Y-%m-%dT00'))
time
time <- c(time,as.Date('2013-03-24'))
time
length(time) == 2
sprintf('%s[(%s:00:00Z):1:(%s:00:00Z)]',
url, strftime(time[1], '%Y-%m-%dT00'),
strftime(time[2], '%Y-%m-%dT00'))
'http://coastwatch.pfeg.noaa.gov/erddap/griddap/jplL4AvhrrOIv1fv2.nc?analysed_sst[(2015-04-01T00:00:00Z):1:(2015-04-01T00:00:00Z)][(60):1:(10)][(-90):1:(-30)]'
lon
lat
sprintf('%s[(%s):1:(%s)][(%s):1:(%s)]',
url, lat[2], lat[1], lon[1], lon[2])
'http://coastwatch.pfeg.noaa.gov/erddap/griddap/jplL4AvhrrOIv1fv2.nc?analysed_sst[(2015-04-01T00:00:00Z):1:(2015-04-01T00:00:00Z)][(60):1:(10)][(-90):1:(-30)]'
expts = data.frame(
start=c(as.Date('1981-09-01')),
end=c(Sys.Date() + 1),
url=c('http://coastwatch.pfeg.noaa.gov/erddap/griddap/jplL4AvhrrOIv1fv2.nc?analysed_sst'))
if(time[1] < expts$start[1])
stop('Data begins at %s and is not available at %s.',
strftime(expts$start[1], '%d %b %Y'),
strftime(time[1], '%d %b %Y'))
if(time[1] > expts$end[nrow(expts)])
stop('Data ends at %s and is not available at %s.',
strftime(expts$end[nrow(expts)], '%d %b %Y'),
strftime(time[1], '%d %b %Y'))
for(i in seq(nrow(expts))) {
if((time[1] >= expts$start[i]) & (time[1] <= expts$end[i]))
url = expts$url[i]
}
## Add the variables.
#for(var in vars)
#  url = sprintf('%svar=%s&', url, var)
## Add the time domain.
if(length(time) == 2){
url = sprintf('%s[(%s:00:00Z):1:(%s:00:00Z)]',
url, strftime(time[1], '%Y-%m-%dT00'),
strftime(time[2], '%Y-%m-%dT00'))
} else if(length(time)==1){
url = sprintf('%s[(%s:00:00Z):1:(%s:00:00Z)]',
url, strftime(time[1], '%Y-%m-%dT00'),
strftime(time[1], '%Y-%m-%dT00'))
}
## Add the spatial domain.
url = sprintf('%s[(%s):1:(%s)][(%s):1:(%s)]',
url, lat[2], lat[1], lon[1], lon[2])
url
'http://coastwatch.pfeg.noaa.gov/erddap/griddap/jplL4AvhrrOIv1fv2.nc?analysed_sst[(2015-04-01T00:00:00Z):1:(2015-04-01T00:00:00Z)][(60):1:(10)][(-90):1:(-30)]'
filename
filename = 'trysst.nc'
download.file(url, filename, method = 'curl')
url
expts = data.frame(
start=c(as.Date('1981-09-01')),
end=c(Sys.Date() + 1),
url=c('http://coastwatch.pfeg.noaa.gov/erddap/griddap/jplL4AvhrrOIv1fv2.nc?analysed_sst'))
if(time[1] < expts$start[1])
stop('Data begins at %s and is not available at %s.',
strftime(expts$start[1], '%d %b %Y'),
strftime(time[1], '%d %b %Y'))
if(time[1] > expts$end[nrow(expts)])
stop('Data ends at %s and is not available at %s.',
strftime(expts$end[nrow(expts)], '%d %b %Y'),
strftime(time[1], '%d %b %Y'))
for(i in seq(nrow(expts))) {
if((time[1] >= expts$start[i]) & (time[1] <= expts$end[i]))
url = expts$url[i]
}
## Add the variables.
#for(var in vars)
#  url = sprintf('%svar=%s&', url, var)
## Add the time domain.
if(length(time) == 2){
url = sprintf('%s[(%s:00:00Z):1:(%s:00:00Z)]',
url, strftime(time[1], '%Y-%m-%dT00'),
strftime(time[2], '%Y-%m-%dT00'))
} else if(length(time)==1){
url = sprintf('%s[(%s:00:00Z):1:(%s:00:00Z)]',
url, strftime(time[1], '%Y-%m-%dT00'),
strftime(time[1], '%Y-%m-%dT00'))
}
## Add the spatial domain.
url = sprintf('%s[(%s):1:(%s)][(%s):1:(%s)]',
url, lat[1], lat[2], lon[1], lon[2])
url
download.file(url, filename, method = 'curl')
?download.file
download.file(url, filename, method = 'internal')
?Sys.glob
download.file(url, filename, method = 'auto')
install.packages('framed')
library(rmarkdown)
?"rmarkdown"
614/60000
588/60000
setwd('~/Documents/WHOI/RData/WhiteSharks/2013/121325/')
sst <- read.table(paste(ptt,'-SST.csv', sep=''), sep=',',header=T,blank.lines.skip=F, skip = 0)
ptt=121325
sst <- read.table(paste(ptt,'-SST.csv', sep=''), sep=',',header=T,blank.lines.skip=F, skip = 0)
str(sst)
tag <- as.POSIXct(paste(iniloc[1,1], '/', iniloc[1,2], '/', iniloc[1,3], sep=''), format = '%d/%m/%Y')
data("countriesLow")
devtools::load_all("~/Documents/WHOI/RCode/hmmwoa")
data("countriesLow")
devtools::load_all("~/Documents/WHOI/RCode/hmmwoa")
data("countriesLow")
ptt <- 121325
iniloc <- data.frame(matrix(c(3, 3, 2013, 30.3917, -81.3802,
31, 8, 2013, 30.668, -79.972), nrow = 2, ncol = 5, byrow = T))
colnames(iniloc) = list('day','month','year','lat','lon')
lon = c(-90, -40)
lat = c(10, 55)
pdt <- extract.pdt(pdt)
dateVec <- as.Date(seq(tag, pop, by = 'day'))
tag <- as.POSIXct(paste(iniloc[1,1], '/', iniloc[1,2], '/', iniloc[1,3], sep=''), format = '%d/%m/%Y')
pop <- as.POSIXct(paste(iniloc[2,1], '/', iniloc[2,2], '/', iniloc[2,3], sep=''), format = '%d/%m/%Y')
dateVec <- as.Date(seq(tag, pop, by = 'day'))
tag <- as.POSIXct(paste(iniloc[1,1], '/', iniloc[1,2], '/', iniloc[1,3], sep=''), format = '%d/%m/%Y')
pop <- as.POSIXct(paste(iniloc[2,1], '/', iniloc[2,2], '/', iniloc[2,3], sep=''), format = '%d/%m/%Y')
findDateFormat <- function(dateVec){
# Function to determine the date format of a given vector of dates
#' @param datevec is a vector of dates as in those from -Histos or
#'        -PDTs from WC tags
#' @return dateformat is character string used as input to strptime(format = dateformat)
dateformat = '%Y-%m-%d %H:%M:%S'
ddates = as.POSIXct(strptime(as.character(dateVec), format = dateformat)) #reads dates as dates
if (is.na(ddates[1])){
dateformat = '%H:%M:%S %d-%b-%Y'
ddates = as.POSIXct(strptime(as.character(dateVec), format = dateformat)) #reads dates as dates
if (is.na(ddates[1])){
dateformat = '%m-%d-%y %H:%M'
ddates = as.POSIXct(strptime(as.character(dateVec), format = dateformat)) #reads dates as dates
if (is.na(ddates[1])){
dateformat = '%m/%d/%y %H:%M'
ddates = as.POSIXct(strptime(as.character(dateVec), format = dateformat)) #reads dates as dates
if (is.na(ddates[1])){
dateformat = '%m/%d/%Y'
ddates = as.POSIXct(strptime(as.character(dateVec), format = dateformat)) #reads dates as dates
if (is.na(ddates[1])){
dateformat = '%H:%M:%S %d-%b-%Y'
ddates = as.POSIXct(strptime(as.character(dateVec), format = dateformat)) #reads dates as dates
if(is.na(ddates[1])){
stop('No correct date format was found.')
} else {}
} else {}
} else {}
} else {}
} else {}
} else {}
dateformat #return dateformat variable
}
dts <- as.POSIXct(sst$Date, format = findDateFormat(sst$Date))
head(dts)
d1 <- as.POSIXct('1900-01-02') - as.POSIXct('1900-01-01')
didx <- dts >= (tag + d1) & dts <= (pop - d1)
didx
tag
pop
dts
dts <- as.POSIXct(sst$Date, format = findDateFormat(sst$Date))
dts
sst <- read.table(paste(ptt,'-SST.csv', sep=''), sep=',',header=T,blank.lines.skip=F, skip = 0)
tag <- as.POSIXct(paste(iniloc[1,1], '/', iniloc[1,2], '/', iniloc[1,3], sep=''), format = '%d/%m/%Y')
pop <- as.POSIXct(paste(iniloc[2,1], '/', iniloc[2,2], '/', iniloc[2,3], sep=''), format = '%d/%m/%Y')
dts <- as.POSIXct(sst$Date, format = findDateFormat(sst$Date))
dts
d1 <- as.POSIXct('1900-01-02') - as.POSIXct('1900-01-01')
didx <- dts >= (tag + d1) & dts <= (pop - d1)
didx
sst <- sst[didx,]
sst
get.oi.sst <- function(lon, lat, time, filename='', download.file=TRUE, dir = getwd()) {
#' Downloads data from the HYCOM + NCODA Global 1/12 Degree Analysis.
#'
#' The method may return before the download is completed. It will continue
#' to display progress  until the download completes.
#'
#' Ideally download.file (default method) would be used instead of curl (optional), but this does not
#' seem to work on some platforms.
#'
#' @param lon An vector of length 2 with the minimum and maximum longitude.
#' @param lat An vector of length 2 with the minimum and maximum latitude.
#' @param time An vector of length 2 with the minimum and maximum times.
#' @param vars A list of variables to download. This should only contain
#' 'sst' 'anom' 'err' but is not checked for errors
#' @param include_latlon Should the array of latitude and longitude values be
#' included?
#' @param filename An optional filename. If provided, then the data is
#' downloaded to that file. Otherwise the data is not downloaded.
#' @param download.file Should use the default download.file function to query
#' the server and download or use the optional curl function. Some users may
#' need to use curl in order to get this to work.
#' @param: dir is directory where nc files should be downloaded to. default is
#' current working directory. if enter a directory that doesn't exist, it will
#' be created.
#' @param type indicates type of HYCOM product to download. 'r' is reanalysis
#'        and 'a' is analysis. see https://hycom.org/dataserver for details.
#' @return The url used to extract the requested data from the NetCDF subset
#' service.
## Function originally written for R by Ben Jones (WHOI) and modified by Camrin
## Braun and Ben Galuardi.
require(ncdf)
dir.create(file.path(dir), recursive = TRUE)
setwd(dir)
## Set the base URL based on the start date. If the ending date exceeds the
## period for this experiment, then print a warning and truncate the output
## early.
expts = data.frame(
start=c(as.Date('1981-09-01')),
end=c(Sys.Date() + 1),
url=c('http://coastwatch.pfeg.noaa.gov/erddap/griddap/jplL4AvhrrOIv1fv2.nc?analysed_sst'))
if(time[1] < expts$start[1])
stop('Data begins at %s and is not available at %s.',
strftime(expts$start[1], '%d %b %Y'),
strftime(time[1], '%d %b %Y'))
if(time[1] > expts$end[nrow(expts)])
stop('Data ends at %s and is not available at %s.',
strftime(expts$end[nrow(expts)], '%d %b %Y'),
strftime(time[1], '%d %b %Y'))
for(i in seq(nrow(expts))) {
if((time[1] >= expts$start[i]) & (time[1] <= expts$end[i]))
url = expts$url[i]
}
## Add the time domain.
if(length(time) == 2){
url = sprintf('%s[(%s:00:00Z):1:(%s:00:00Z)]',
url, strftime(time[1], '%Y-%m-%dT00'),
strftime(time[2], '%Y-%m-%dT00'))
} else if(length(time)==1){
url = sprintf('%s[(%s:00:00Z):1:(%s:00:00Z)]',
url, strftime(time[1], '%Y-%m-%dT00'),
strftime(time[1], '%Y-%m-%dT00'))
}
## Add the spatial domain.
url = sprintf('%s[(%s):1:(%s)][(%s):1:(%s)]',
url, lat[1], lat[2], lon[1], lon[2])
## Download the data if a filename was provided.
if(filename != ''){
if(download.file == TRUE){
download.file(url, filename, method = 'auto')
} else if(download.file == FALSE){
system(sprintf('curl -o "%s" "%s"', filename, url))
}
}
return(url)
}
lon
lat
time
time <- as.Date(udates[i])
udates <- unique(sst$Date)
i=1
time <- as.Date(udates[i])
time
udates
udates <- unique(as.Date(sst$Date))
time <- as.Date(udates[i])
time
str(udates)
udates <- unique(sst$Date)
str(udates)
udates <- unique(as.Date(dts))
str(udates)
time <- as.Date(udates[i])
time
get.oi.sst(lon,lat,time,filename=paste(ptt,'_-',time,'.nc',sep=''), download.file=TRUE, dir=sst.dir) # filenames based on dates from above
sst.dir <- '~/Documents/WHOI/RData/sst/'
get.oi.sst(lon,lat,time,filename=paste(ptt,'_-',time,'.nc',sep=''), download.file=TRUE, dir=sst.dir) # filenames based on dates from above
get.oi.sst(lon,lat,time,filename=paste(ptt,'_-',time,'.nc',sep=''), download.file=TRUE, dir=sst.dir) # filenames based on dates from above
time <- c(as.Date(udates[1]), as.Date(udates[5]))
get.oi.sst(lon,lat,time,filename=paste(ptt,'_-',time,'.nc',sep=''), download.file=TRUE, dir=sst.dir) # filenames based on dates from above
