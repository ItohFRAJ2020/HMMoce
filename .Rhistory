#' @param: dir is directory where nc files should be downloaded to. default is
#' current working directory. if enter a directory that doesn't exist, it will
#' be created.
#' @return The url used to extract the requested data from the NetCDF subset
#' service.
## Function originally written for R by Ben Jones (WHOI) and modified by Camrin
## Braun and Ben Galuardi.
require(ncdf)
dir.create(file.path(dir),recursive=TRUE)
setwd(dir)
## Set the base URL based on the start date. If the ending date exceeds the
## period for this experiment, then print a warning and truncate the output
## early.
expts = data.frame(
start=c(as.Date('2008-09-01'), as.Date('2009-05-01'),
as.Date('2011-01-01'), as.Date('2013-08-01'),
as.Date('2014-04-01')),
end=c(as.Date('2009-04-30'), as.Date('2010-12-31'),
as.Date('2013-07-31'), as.Date('2014-03-31'),
Sys.Date() + 1),
url=c('http://ncss.hycom.org/thredds/ncss/GLBa0.08/expt_90.6?',
'http://ncss.hycom.org/thredds/ncss/GLBa0.08/expt_90.8?',
'http://ncss.hycom.org/thredds/ncss/GLBa0.08/expt_90.9?',
'http://ncss.hycom.org/thredds/ncss/GLBa0.08/expt_91.0?',
'http://ncss.hycom.org/thredds/ncss/GLBa0.08/expt_91.1?'))
if(time[1] < expts$start[1])
stop('Data begins at %s and is not available at %s.',
strftime(expts$start[1], '%d %b %Y'),
strftime(time[1], '%d %b %Y'))
if(time[1] > expts$end[nrow(expts)])
stop('Data ends at %s and is not available at %s.',
strftime(expts$end[nrow(expts)], '%d %b %Y'),
strftime(time[1], '%d %b %Y'))
for(i in seq(nrow(expts))) {
if((time[1] > expts$start[i]) & (time[1] <= expts$end[i]))
url = expts$url[i]
}
## Add the variables.
for(var in vars)
url = sprintf('%svar=%s&', url, var)
## Add the spatial domain.
url = sprintf('%snorth=%f&west=%f&east=%f&south=%f&horizStride=1&',
url, lat[2], lon[1], lon[2], lat[1])
## Add the time domain.
if(length(time)==2){
url = sprintf('%stime_start=%s%%3A00%%3A00Z&time_end=%s%%3A00%%3A00Z&timeStride=1&',
url, strftime(time[1], '%Y-%m-%dT00', start_time),
strftime(time[2], '%Y-%m-%dT00', end_time))
} else if(length(time)==1){
url = sprintf('%stime_start=%s%%3A00%%3A00Z&time_end=%s%%3A00%%3A00Z&timeStride=1&',
url, strftime(time[1], '%Y-%m-%dT00'),
strftime(time[1], '%Y-%m-%dT00'))
}
## Add the lat-lon points if requested.
if(include_latlon)
url = sprintf('%saddLatLon=true&', url)
## Finish the URL.
url = sprintf('%sdisableProjSubset=on&vertCoord=&accept=netcdf', url)
## Download the data if a filename was provided.
if(filename != ''){
if(download.file==TRUE){
download.file(url,filename)
} else if(download.file==FALSE){
system(sprintf('curl -o "%s" "%s"', filename, url))
}
}
return(url)
}
findDateFormat <- function(dateVec){
# Function to determine the date format of a given vector of dates
# Input:
#   DATEVEC is a vector of dates as in those from -Histos or -PDTs from WC satellite tags
# Output:
#   DATEFORMAT is character string used as input to strptime(format = dateformat)
#--------------------------------------------------------------------------
# - Written by Camrin Braun in R, December 24, 2014. cbraun@whoi.edu
#--------------------------------------------------------------------------
dateformat = '%Y-%m-%d %H:%M:%S'
ddates = as.POSIXct(strptime(as.character(dateVec),format = dateformat)) #reads dates as dates
if (is.na(ddates[1])){
dateformat = '%H:%M:%S %d-%b-%Y'
ddates = as.POSIXct(strptime(as.character(dateVec),format = dateformat)) #reads dates as dates
if (is.na(ddates[1])){
dateformat = '%m-%d-%y %H:%M'
ddates = as.POSIXct(strptime(as.character(dateVec),format = dateformat)) #reads dates as dates
if (is.na(ddates[1])){
dateformat = '%m/%d/%y %H:%M'
ddates = as.POSIXct(strptime(as.character(dateVec),format = dateformat)) #reads dates as dates
if (is.na(ddates[1])){
dateformat = '%m/%d/%Y'
ddates = as.POSIXct(strptime(as.character(dateVec),format = dateformat)) #reads dates as dates
if (is.na(ddates[1])){
dateformat = '%H:%M:%S %d-%b-%Y'
ddates = as.POSIXct(strptime(as.character(dateVec),format = dateformat)) #reads dates as dates
if(is.na(ddates[1])){
stop('No correct date format was found.')
} else {}
} else {}
} else {}
} else {}
} else {}
} else {}
dateformat #return dateformat variable
}
setwd('~/Documents/WHOI/RData/WhiteSharks/2013/121325/')
ptt = 121325
data = read.table('121325-PDTs.csv',sep=',',header=T,blank.lines.skip=F)
pdt = extract.pdt(data)
# download daily hycom products
#time <- c(as.Date(min(pdt$Date)),as.Date(max(pdt$Date)))
lon = c(-90, -40)
lat = c(10, 50)
ohc.dir <- '~/Documents/WHOI/RData/HYCOM/Lydia/'
udates <- unique(as.Date(pdt$Date))
i=128
time <- as.Date(udates[i])
time
for(i in 128:length(udates)){
time <- as.Date(udates[i])
repeat{
get.hycom(lon,lat,time,filename=paste(ptt,'_',time,'.nc',sep=''),download.file=TRUE,dir=ohc.dir) # filenames based on dates from above
err <- try(open.ncdf(paste(ohc.dir,ptt,'_',time,'.nc',sep='')),silent=T)
if(class(err) != 'try-error') break
}
}
for(i in 128:length(udates)){
time <- as.Date(udates[i])
#repeat{
get.hycom(lon,lat,time,filename=paste(ptt,'_',time,'.nc',sep=''),download.file=TRUE,dir=ohc.dir) # filenames based on dates from above
# err <- try(
#nc <- open.ncdf(paste(ohc.dir,ptt,'_',time,'.nc',sep='')),silent=T)
#if(class(err) != 'try-error') break
# }
}
lon
lat
time
paste(ptt,'_',time,'.nc',sep='')
vars=c('temperature')
include_latlon=TRUE
download.file=TRUE
dir=ohc.dir
require(ncdf)
dir.create(file.path(dir),recursive=TRUE)
setwd(dir)
expts = data.frame(
start=c(as.Date('2008-09-01'), as.Date('2009-05-01'),
as.Date('2011-01-01'), as.Date('2013-08-01'),
as.Date('2014-04-01')),
end=c(as.Date('2009-04-30'), as.Date('2010-12-31'),
as.Date('2013-07-31'), as.Date('2014-03-31'),
Sys.Date() + 1),
url=c('http://ncss.hycom.org/thredds/ncss/GLBa0.08/expt_90.6?',
'http://ncss.hycom.org/thredds/ncss/GLBa0.08/expt_90.8?',
'http://ncss.hycom.org/thredds/ncss/GLBa0.08/expt_90.9?',
'http://ncss.hycom.org/thredds/ncss/GLBa0.08/expt_91.0?',
'http://ncss.hycom.org/thredds/ncss/GLBa0.08/expt_91.1?'))
if(time[1] < expts$start[1])
stop('Data begins at %s and is not available at %s.',
strftime(expts$start[1], '%d %b %Y'),
strftime(time[1], '%d %b %Y'))
if(time[1] > expts$end[nrow(expts)])
stop('Data ends at %s and is not available at %s.',
strftime(expts$end[nrow(expts)], '%d %b %Y'),
strftime(time[1], '%d %b %Y'))
for(i in seq(nrow(expts))) {
if((time[1] > expts$start[i]) & (time[1] <= expts$end[i]))
url = expts$url[i]
}
for(var in vars)
url = sprintf('%svar=%s&', url, var)
var
vars
sprintf('%svar=%s&', url, var)
url
(time[1] > expts$start[i])
time
expts
seq(nrow(expts))
i=4
(time[1] > expts$start[i])
expts$start[i]
get.hycom = function(lon, lat, time, vars=c('temperature'), include_latlon=TRUE,
filename='',download.file=TRUE, dir = getwd()) {
#' Downloads data from the HYCOM + NCODA Global 1/12 Degree Analysis.
#'
#' The method may return before the download is completed. It will continue
#' to display a progress bar until the download completes.
#'
#' Ideally download.file (default method) would be used instead of curl (optional), but this does not
#' seem to work on some platforms.
#'
#' @param lon An vector of length 2 with the minimum and maximum longitude.
#' @param lat An vector of length 2 with the minimum and maximum latitude.
#' @param time An vector of length 2 with the minimum and maximum times.
#' @param vars A list of variables to download. This should only contain
#' 'emp', 'mld', 'mlp', qtot', 'ssh', 'surface_salinity_trend',
#' 'surface_temperature_trend', 'salinity', 'temperature', 'u', and 'v', but is
#' not checked for errors.
#' @param include_latlon Should the array of latitude and longitude values be
#' included?
#' @param filename An optional filename. If provided, then the data is
#' downloaded to that file. Otherwise the data is not downloaded.
#' @param download.file Should use the default download.file function to query
#' the server and download or use the optional curl function. Some users may
#' need to use curl in order to get this to work.
#' @param: dir is directory where nc files should be downloaded to. default is
#' current working directory. if enter a directory that doesn't exist, it will
#' be created.
#' @return The url used to extract the requested data from the NetCDF subset
#' service.
## Function originally written for R by Ben Jones (WHOI) and modified by Camrin
## Braun and Ben Galuardi.
require(ncdf)
dir.create(file.path(dir),recursive=TRUE)
setwd(dir)
## Set the base URL based on the start date. If the ending date exceeds the
## period for this experiment, then print a warning and truncate the output
## early.
expts = data.frame(
start=c(as.Date('2008-09-01'), as.Date('2009-05-01'),
as.Date('2011-01-01'), as.Date('2013-08-01'),
as.Date('2014-04-01')),
end=c(as.Date('2009-04-30'), as.Date('2010-12-31'),
as.Date('2013-07-31'), as.Date('2014-03-31'),
Sys.Date() + 1),
url=c('http://ncss.hycom.org/thredds/ncss/GLBa0.08/expt_90.6?',
'http://ncss.hycom.org/thredds/ncss/GLBa0.08/expt_90.8?',
'http://ncss.hycom.org/thredds/ncss/GLBa0.08/expt_90.9?',
'http://ncss.hycom.org/thredds/ncss/GLBa0.08/expt_91.0?',
'http://ncss.hycom.org/thredds/ncss/GLBa0.08/expt_91.1?'))
if(time[1] < expts$start[1])
stop('Data begins at %s and is not available at %s.',
strftime(expts$start[1], '%d %b %Y'),
strftime(time[1], '%d %b %Y'))
if(time[1] > expts$end[nrow(expts)])
stop('Data ends at %s and is not available at %s.',
strftime(expts$end[nrow(expts)], '%d %b %Y'),
strftime(time[1], '%d %b %Y'))
for(i in seq(nrow(expts))) {
if((time[1] >= expts$start[i]) & (time[1] <= expts$end[i]))
url = expts$url[i]
}
## Add the variables.
for(var in vars)
url = sprintf('%svar=%s&', url, var)
## Add the spatial domain.
url = sprintf('%snorth=%f&west=%f&east=%f&south=%f&horizStride=1&',
url, lat[2], lon[1], lon[2], lat[1])
## Add the time domain.
if(length(time)==2){
url = sprintf('%stime_start=%s%%3A00%%3A00Z&time_end=%s%%3A00%%3A00Z&timeStride=1&',
url, strftime(time[1], '%Y-%m-%dT00', start_time),
strftime(time[2], '%Y-%m-%dT00', end_time))
} else if(length(time)==1){
url = sprintf('%stime_start=%s%%3A00%%3A00Z&time_end=%s%%3A00%%3A00Z&timeStride=1&',
url, strftime(time[1], '%Y-%m-%dT00'),
strftime(time[1], '%Y-%m-%dT00'))
}
## Add the lat-lon points if requested.
if(include_latlon)
url = sprintf('%saddLatLon=true&', url)
## Finish the URL.
url = sprintf('%sdisableProjSubset=on&vertCoord=&accept=netcdf', url)
## Download the data if a filename was provided.
if(filename != ''){
if(download.file==TRUE){
download.file(url,filename)
} else if(download.file==FALSE){
system(sprintf('curl -o "%s" "%s"', filename, url))
}
}
return(url)
}
i=128
time <- as.Date(udates[i])
time
get.hycom(lon,lat,time,filename=paste(ptt,'_',time,'.nc',sep=''),download.file=TRUE,dir=ohc.dir) # filenames based on dates from above
get.hycom(lon,lat,time,filename=paste(ptt,'_',time,'.nc',sep=''),download.file=TRUE,dir=ohc.dir) # filenames based on dates from above
expts[4]
expts[4,]
expts
get.hycom = function(lon, lat, time, vars=c('temperature'), include_latlon=TRUE,
filename='',download.file=TRUE, dir = getwd()) {
#' Downloads data from the HYCOM + NCODA Global 1/12 Degree Analysis.
#'
#' The method may return before the download is completed. It will continue
#' to display a progress bar until the download completes.
#'
#' Ideally download.file (default method) would be used instead of curl (optional), but this does not
#' seem to work on some platforms.
#'
#' @param lon An vector of length 2 with the minimum and maximum longitude.
#' @param lat An vector of length 2 with the minimum and maximum latitude.
#' @param time An vector of length 2 with the minimum and maximum times.
#' @param vars A list of variables to download. This should only contain
#' 'emp', 'mld', 'mlp', qtot', 'ssh', 'surface_salinity_trend',
#' 'surface_temperature_trend', 'salinity', 'temperature', 'u', and 'v', but is
#' not checked for errors.
#' @param include_latlon Should the array of latitude and longitude values be
#' included?
#' @param filename An optional filename. If provided, then the data is
#' downloaded to that file. Otherwise the data is not downloaded.
#' @param download.file Should use the default download.file function to query
#' the server and download or use the optional curl function. Some users may
#' need to use curl in order to get this to work.
#' @param: dir is directory where nc files should be downloaded to. default is
#' current working directory. if enter a directory that doesn't exist, it will
#' be created.
#' @return The url used to extract the requested data from the NetCDF subset
#' service.
## Function originally written for R by Ben Jones (WHOI) and modified by Camrin
## Braun and Ben Galuardi.
require(ncdf)
dir.create(file.path(dir),recursive=TRUE)
setwd(dir)
## Set the base URL based on the start date. If the ending date exceeds the
## period for this experiment, then print a warning and truncate the output
## early.
expts = data.frame(
start=c(as.Date('2008-09-19'), as.Date('2009-05-07'),
as.Date('2011-01-03'), as.Date('2013-08-21'),
as.Date('2014-04-05')),
end=c(as.Date('2009-05-06'), as.Date('2011-01-02'),
as.Date('2013-08-20'), as.Date('2014-04-04'),
Sys.Date() + 1),
url=c('http://ncss.hycom.org/thredds/ncss/GLBa0.08/expt_90.6?',
'http://ncss.hycom.org/thredds/ncss/GLBa0.08/expt_90.8?',
'http://ncss.hycom.org/thredds/ncss/GLBa0.08/expt_90.9?',
'http://ncss.hycom.org/thredds/ncss/GLBa0.08/expt_91.0?',
'http://ncss.hycom.org/thredds/ncss/GLBa0.08/expt_91.1?'))
if(time[1] < expts$start[1])
stop('Data begins at %s and is not available at %s.',
strftime(expts$start[1], '%d %b %Y'),
strftime(time[1], '%d %b %Y'))
if(time[1] > expts$end[nrow(expts)])
stop('Data ends at %s and is not available at %s.',
strftime(expts$end[nrow(expts)], '%d %b %Y'),
strftime(time[1], '%d %b %Y'))
for(i in seq(nrow(expts))) {
if((time[1] >= expts$start[i]) & (time[1] <= expts$end[i]))
url = expts$url[i]
}
## Add the variables.
for(var in vars)
url = sprintf('%svar=%s&', url, var)
## Add the spatial domain.
url = sprintf('%snorth=%f&west=%f&east=%f&south=%f&horizStride=1&',
url, lat[2], lon[1], lon[2], lat[1])
## Add the time domain.
if(length(time)==2){
url = sprintf('%stime_start=%s%%3A00%%3A00Z&time_end=%s%%3A00%%3A00Z&timeStride=1&',
url, strftime(time[1], '%Y-%m-%dT00', start_time),
strftime(time[2], '%Y-%m-%dT00', end_time))
} else if(length(time)==1){
url = sprintf('%stime_start=%s%%3A00%%3A00Z&time_end=%s%%3A00%%3A00Z&timeStride=1&',
url, strftime(time[1], '%Y-%m-%dT00'),
strftime(time[1], '%Y-%m-%dT00'))
}
## Add the lat-lon points if requested.
if(include_latlon)
url = sprintf('%saddLatLon=true&', url)
## Finish the URL.
url = sprintf('%sdisableProjSubset=on&vertCoord=&accept=netcdf', url)
## Download the data if a filename was provided.
if(filename != ''){
if(download.file==TRUE){
download.file(url,filename)
} else if(download.file==FALSE){
system(sprintf('curl -o "%s" "%s"', filename, url))
}
}
return(url)
}
i
time
get.hycom(lon,lat,time,filename=paste(ptt,'_',time,'.nc',sep=''),download.file=TRUE,dir=ohc.dir) # filenames based on dates from above
i=129
time <- as.Date(udates[i])
err <- try(open.ncdf(paste(ohc.dir,ptt,'_',time,'.nc',sep='')),silent=T)
?tryCatch
time
err <- open.ncdf(paste(ohc.dir,ptt,'_',time,'.nc',sep=''))
tryCatch({
err <- open.ncdf(paste(ohc.dir,ptt,'_',time,'.nc',sep=''))
}, error=function(e){})
tryCatch({
err <- try(open.ncdf(paste(ohc.dir,ptt,'_',time,'.nc',sep='')),silent=T)
}, error=function(e){})
for(i in 129:length(udates)){
time <- as.Date(udates[i])
repeat{
get.hycom(lon,lat,time,filename=paste(ptt,'_',time,'.nc',sep=''),download.file=TRUE,dir=ohc.dir) # filenames based on dates from above
#err <- try(open.ncdf(paste(ohc.dir,ptt,'_',time,'.nc',sep='')),silent=T)
tryCatch({
err <- try(open.ncdf(paste(ohc.dir,ptt,'_',time,'.nc',sep='')),silent=T)
}, error=function(e){})
if(class(err) != 'try-error') break
}
}
tail(udates)
calc.ohc = function(pdt,isotherm='',ohc.dir,ptt){
# compare tag data to ohc map and calculate likelihoods
#' @param: tagdata is variable containing tag-collected PDT data
#' @param: time is vector of unique dates (daily) used to step
#' through the integration / ohc calculations
#' @param: isotherm is default '' in which isotherm is calculated
#' on the fly based on daily shark data. Otherwise, numeric isotherm
#' constraint can be specified.
#' @param: ohc.dir is local directory where get.hycom downloads are
#' stored.
#' @return: likelihood is array of likelihood surfaces representing
#' matches between daily tag-based ohc and hycom ohc maps
require(ncdf); require(abind)
# constants for OHC calc
cp = 3.993 # kJ/kg*C
rho = 1025 # kg/m3
# calculate midpoint of tag-based min/max temps
pdt$MidTemp <- (pdt$MaxTemp + pdt$MinTemp) / 2
udates <- unique(as.Date(pdt$Date))
for(i in 1:length(udates)){
# define time based on tag data
time <- as.Date(udates[i])
nc = open.ncdf(paste(ohc.dir,ptt,'_',time,'.nc',sep=''))
dat = get.var.ncdf(nc, 'temperature')
pdt.i <- pdt[which(as.Date(pdt$Date)==time),]
if(isotherm == ''){
# calculate daily isotherm based on tag data
isotherm = min(pdt.i$MidTemp,na.rm=T)
}
dat[dat<isotherm] = NA
# Perform hycom integration
dat = dat - isotherm
ohc = cp*rho*apply(dat, 1:2, sum, na.rm=T)/10000
# perform tag data integration
tag = pdt.i$MidTemp - isotherm
tag.ohc = cp*rho*sum(tag,na.rm=T)/10000
# compare hycom to that day's tag-based ohc
lik = dnorm(ohc, tag.ohc, sdx) # how to represent sd of tag-based ohc?
# result should be array of likelihood surfaces
if(i==time[1]){
likelihood = as.array(lik)
# also get lon, lat, depth, variables in case we need them later
lon.length = get.var.ncdf(nc, 'X')
lat.length = get.var.ncdf(nc, 'Y')
lon = seq(lon[1], lon[2], length = length(lon.length))
lat = seq(lat[1], lat[2], length = length(lat.length))
depth = get.var.ncdf(nc, 'Depth')
} else{
likelihood = abind(likelihood,lik,along=3)
}
}
# maybe add plot option in which each day is added to a pdf on file
# return ohc likelihood surfaces as an array
return(likelihood)
}
likelihood = calc.ohc(pdt,ohc.dir=ohc.dir,ptt=121325)
calc.ohc = function(pdt,isotherm='',ohc.dir,ptt,sdx){
# compare tag data to ohc map and calculate likelihoods
#' @param: tagdata is variable containing tag-collected PDT data
#' @param: time is vector of unique dates (daily) used to step
#' through the integration / ohc calculations
#' @param: isotherm is default '' in which isotherm is calculated
#' on the fly based on daily shark data. Otherwise, numeric isotherm
#' constraint can be specified.
#' @param: ohc.dir is local directory where get.hycom downloads are
#' stored.
#' @return: likelihood is array of likelihood surfaces representing
#' matches between daily tag-based ohc and hycom ohc maps
require(ncdf); require(abind)
# constants for OHC calc
cp = 3.993 # kJ/kg*C
rho = 1025 # kg/m3
# calculate midpoint of tag-based min/max temps
pdt$MidTemp <- (pdt$MaxTemp + pdt$MinTemp) / 2
udates <- unique(as.Date(pdt$Date))
for(i in 1:length(udates)){
# define time based on tag data
time <- as.Date(udates[i])
nc = open.ncdf(paste(ohc.dir,ptt,'_',time,'.nc',sep=''))
dat = get.var.ncdf(nc, 'temperature')
pdt.i <- pdt[which(as.Date(pdt$Date)==time),]
if(isotherm == ''){
# calculate daily isotherm based on tag data
isotherm = min(pdt.i$MidTemp,na.rm=T)
}
dat[dat<isotherm] = NA
# Perform hycom integration
dat = dat - isotherm
ohc = cp*rho*apply(dat, 1:2, sum, na.rm=T)/10000
# perform tag data integration
tag = pdt.i$MidTemp - isotherm
tag.ohc = cp*rho*sum(tag,na.rm=T)/10000
# compare hycom to that day's tag-based ohc
lik = dnorm(ohc, tag.ohc, sdx) # how to represent sd of tag-based ohc?
# result should be array of likelihood surfaces
if(i==time[1]){
likelihood = as.array(lik)
# also get lon, lat, depth, variables in case we need them later
lon.length = get.var.ncdf(nc, 'X')
lat.length = get.var.ncdf(nc, 'Y')
lon = seq(lon[1], lon[2], length = length(lon.length))
lat = seq(lat[1], lat[2], length = length(lat.length))
depth = get.var.ncdf(nc, 'Depth')
} else{
likelihood = abind(likelihood,lik,along=3)
}
}
# maybe add plot option in which each day is added to a pdf on file
# return ohc likelihood surfaces as an array
return(likelihood)
}
