<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />

<meta name="viewport" content="width=device-width, initial-scale=1">

<meta name="author" content="Camrin Braun, Benjamin Galuardi, Simon Thorrold" />

<meta name="date" content="2016-12-06" />

<title>A user’s guide to improved analysis of marine animal movement data using HMMoce</title>



<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>



<link href="data:text/css;charset=utf-8,body%20%7B%0Abackground%2Dcolor%3A%20%23fff%3B%0Amargin%3A%201em%20auto%3B%0Amax%2Dwidth%3A%20700px%3B%0Aoverflow%3A%20visible%3B%0Apadding%2Dleft%3A%202em%3B%0Apadding%2Dright%3A%202em%3B%0Afont%2Dfamily%3A%20%22Open%20Sans%22%2C%20%22Helvetica%20Neue%22%2C%20Helvetica%2C%20Arial%2C%20sans%2Dserif%3B%0Afont%2Dsize%3A%2014px%3B%0Aline%2Dheight%3A%201%2E35%3B%0A%7D%0A%23header%20%7B%0Atext%2Dalign%3A%20center%3B%0A%7D%0A%23TOC%20%7B%0Aclear%3A%20both%3B%0Amargin%3A%200%200%2010px%2010px%3B%0Apadding%3A%204px%3B%0Awidth%3A%20400px%3B%0Aborder%3A%201px%20solid%20%23CCCCCC%3B%0Aborder%2Dradius%3A%205px%3B%0Abackground%2Dcolor%3A%20%23f6f6f6%3B%0Afont%2Dsize%3A%2013px%3B%0Aline%2Dheight%3A%201%2E3%3B%0A%7D%0A%23TOC%20%2Etoctitle%20%7B%0Afont%2Dweight%3A%20bold%3B%0Afont%2Dsize%3A%2015px%3B%0Amargin%2Dleft%3A%205px%3B%0A%7D%0A%23TOC%20ul%20%7B%0Apadding%2Dleft%3A%2040px%3B%0Amargin%2Dleft%3A%20%2D1%2E5em%3B%0Amargin%2Dtop%3A%205px%3B%0Amargin%2Dbottom%3A%205px%3B%0A%7D%0A%23TOC%20ul%20ul%20%7B%0Amargin%2Dleft%3A%20%2D2em%3B%0A%7D%0A%23TOC%20li%20%7B%0Aline%2Dheight%3A%2016px%3B%0A%7D%0Atable%20%7B%0Amargin%3A%201em%20auto%3B%0Aborder%2Dwidth%3A%201px%3B%0Aborder%2Dcolor%3A%20%23DDDDDD%3B%0Aborder%2Dstyle%3A%20outset%3B%0Aborder%2Dcollapse%3A%20collapse%3B%0A%7D%0Atable%20th%20%7B%0Aborder%2Dwidth%3A%202px%3B%0Apadding%3A%205px%3B%0Aborder%2Dstyle%3A%20inset%3B%0A%7D%0Atable%20td%20%7B%0Aborder%2Dwidth%3A%201px%3B%0Aborder%2Dstyle%3A%20inset%3B%0Aline%2Dheight%3A%2018px%3B%0Apadding%3A%205px%205px%3B%0A%7D%0Atable%2C%20table%20th%2C%20table%20td%20%7B%0Aborder%2Dleft%2Dstyle%3A%20none%3B%0Aborder%2Dright%2Dstyle%3A%20none%3B%0A%7D%0Atable%20thead%2C%20table%20tr%2Eeven%20%7B%0Abackground%2Dcolor%3A%20%23f7f7f7%3B%0A%7D%0Ap%20%7B%0Amargin%3A%200%2E5em%200%3B%0A%7D%0Ablockquote%20%7B%0Abackground%2Dcolor%3A%20%23f6f6f6%3B%0Apadding%3A%200%2E25em%200%2E75em%3B%0A%7D%0Ahr%20%7B%0Aborder%2Dstyle%3A%20solid%3B%0Aborder%3A%20none%3B%0Aborder%2Dtop%3A%201px%20solid%20%23777%3B%0Amargin%3A%2028px%200%3B%0A%7D%0Adl%20%7B%0Amargin%2Dleft%3A%200%3B%0A%7D%0Adl%20dd%20%7B%0Amargin%2Dbottom%3A%2013px%3B%0Amargin%2Dleft%3A%2013px%3B%0A%7D%0Adl%20dt%20%7B%0Afont%2Dweight%3A%20bold%3B%0A%7D%0Aul%20%7B%0Amargin%2Dtop%3A%200%3B%0A%7D%0Aul%20li%20%7B%0Alist%2Dstyle%3A%20circle%20outside%3B%0A%7D%0Aul%20ul%20%7B%0Amargin%2Dbottom%3A%200%3B%0A%7D%0Apre%2C%20code%20%7B%0Abackground%2Dcolor%3A%20%23f7f7f7%3B%0Aborder%2Dradius%3A%203px%3B%0Acolor%3A%20%23333%3B%0Awhite%2Dspace%3A%20pre%2Dwrap%3B%20%0A%7D%0Apre%20%7B%0Aborder%2Dradius%3A%203px%3B%0Amargin%3A%205px%200px%2010px%200px%3B%0Apadding%3A%2010px%3B%0A%7D%0Apre%3Anot%28%5Bclass%5D%29%20%7B%0Abackground%2Dcolor%3A%20%23f7f7f7%3B%0A%7D%0Acode%20%7B%0Afont%2Dfamily%3A%20Consolas%2C%20Monaco%2C%20%27Courier%20New%27%2C%20monospace%3B%0Afont%2Dsize%3A%2085%25%3B%0A%7D%0Ap%20%3E%20code%2C%20li%20%3E%20code%20%7B%0Apadding%3A%202px%200px%3B%0A%7D%0Adiv%2Efigure%20%7B%0Atext%2Dalign%3A%20center%3B%0A%7D%0Aimg%20%7B%0Abackground%2Dcolor%3A%20%23FFFFFF%3B%0Apadding%3A%202px%3B%0Aborder%3A%201px%20solid%20%23DDDDDD%3B%0Aborder%2Dradius%3A%203px%3B%0Aborder%3A%201px%20solid%20%23CCCCCC%3B%0Amargin%3A%200%205px%3B%0A%7D%0Ah1%20%7B%0Amargin%2Dtop%3A%200%3B%0Afont%2Dsize%3A%2035px%3B%0Aline%2Dheight%3A%2040px%3B%0A%7D%0Ah2%20%7B%0Aborder%2Dbottom%3A%204px%20solid%20%23f7f7f7%3B%0Apadding%2Dtop%3A%2010px%3B%0Apadding%2Dbottom%3A%202px%3B%0Afont%2Dsize%3A%20145%25%3B%0A%7D%0Ah3%20%7B%0Aborder%2Dbottom%3A%202px%20solid%20%23f7f7f7%3B%0Apadding%2Dtop%3A%2010px%3B%0Afont%2Dsize%3A%20120%25%3B%0A%7D%0Ah4%20%7B%0Aborder%2Dbottom%3A%201px%20solid%20%23f7f7f7%3B%0Amargin%2Dleft%3A%208px%3B%0Afont%2Dsize%3A%20105%25%3B%0A%7D%0Ah5%2C%20h6%20%7B%0Aborder%2Dbottom%3A%201px%20solid%20%23ccc%3B%0Afont%2Dsize%3A%20105%25%3B%0A%7D%0Aa%20%7B%0Acolor%3A%20%230033dd%3B%0Atext%2Ddecoration%3A%20none%3B%0A%7D%0Aa%3Ahover%20%7B%0Acolor%3A%20%236666ff%3B%20%7D%0Aa%3Avisited%20%7B%0Acolor%3A%20%23800080%3B%20%7D%0Aa%3Avisited%3Ahover%20%7B%0Acolor%3A%20%23BB00BB%3B%20%7D%0Aa%5Bhref%5E%3D%22http%3A%22%5D%20%7B%0Atext%2Ddecoration%3A%20underline%3B%20%7D%0Aa%5Bhref%5E%3D%22https%3A%22%5D%20%7B%0Atext%2Ddecoration%3A%20underline%3B%20%7D%0A%0Acode%20%3E%20span%2Ekw%20%7B%20color%3A%20%23555%3B%20font%2Dweight%3A%20bold%3B%20%7D%20%0Acode%20%3E%20span%2Edt%20%7B%20color%3A%20%23902000%3B%20%7D%20%0Acode%20%3E%20span%2Edv%20%7B%20color%3A%20%2340a070%3B%20%7D%20%0Acode%20%3E%20span%2Ebn%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Efl%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Ech%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Est%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Eco%20%7B%20color%3A%20%23888888%3B%20font%2Dstyle%3A%20italic%3B%20%7D%20%0Acode%20%3E%20span%2Eot%20%7B%20color%3A%20%23007020%3B%20%7D%20%0Acode%20%3E%20span%2Eal%20%7B%20color%3A%20%23ff0000%3B%20font%2Dweight%3A%20bold%3B%20%7D%20%0Acode%20%3E%20span%2Efu%20%7B%20color%3A%20%23900%3B%20font%2Dweight%3A%20bold%3B%20%7D%20%20code%20%3E%20span%2Eer%20%7B%20color%3A%20%23a61717%3B%20background%2Dcolor%3A%20%23e3d2d2%3B%20%7D%20%0A" rel="stylesheet" type="text/css" />

</head>

<body>




<h1 class="title toc-ignore">A user’s guide to improved analysis of marine animal movement data using HMMoce</h1>
<h4 class="author"><em>Camrin Braun, Benjamin Galuardi, Simon Thorrold</em></h4>
<h4 class="date"><em>2016-12-06</em></h4>



<div id="summary" class="section level2">
<h2>Summary</h2>
<p>While the number of marine animals being tagged and tracked continues to grow, current satellite tracking techniques largely constrain meaninful inference to largescale movements of surface-dwelling species and are inherently prone to significant error. Hidden Markov models (HMMs) have become increasingly common in the analysis of animal movement data by incorporating underlying behavioral states into movement data. This discretized approach also provides efficient handling of grid-based oceanographic data and likelihood surfaces generated within the package. We present an open-source <code>R</code> package, <code>HMMoce</code>, that uses a novel state-space HMM approach to improve position estimates derived from electronic tags using three-dimensional oceanographic data. We demonstrate <code>HMMoce</code> with example blue shark (<em>Prionace glauca</em>) data that is included in the package. Our findings illustrate how our software leverages all available tag data, along with oceanographic information, to improve position estimates of tagged marine species.</p>
</div>
<div id="introduction" class="section level2">
<h2>Introduction</h2>
<p>There are many approaches to estimating animal movements from various types of tag data. The paradigm in fish tracking has been to use light levels to estimate position, but many species spend considerable time away from the photic zone. Diving behavior, like a typical diel vertical migration exhibited by deep diving swordfish, can render light geolocation useless. Yet, deep diving provides depth-temperature profile data recorded by the archival tag as it samples throughout a tagged individual’s vertical movements. This sampling provides a unique signature through the oceanographic environment that can be leveraged to help constrain position. When combined with other tag-measured data streams like sea surface temperature (SST), light levels and maximum diving depth, we expect a unique combination of oceanographic characteristics to be diagnostic of an animal’s location.</p>
</div>
<div id="how-it-works" class="section level1">
<h1>How it works</h1>
<div id="the-observation-model" class="section level2">
<h2>The Observation Model</h2>
<p>There are 3 main data streams currently collected by archival tags on marine animals: light, SST, depth-temperature profiles. Each of these data streams contains information about the location of the animal in the global ocean; thus, each can be leveraged to inform our estimation of animal movements. Light levels and SST are the current paradigm for fish tracking and are the most straightforward to use for positioning. The depth-temperature profiles are more complex but provide rich information about oceanographic characteristics animals experiences as they move.</p>
<div id="light-likelihood" class="section level3">
<h3>Light likelihood</h3>
<p>In <code>HMMoce</code>, there are currently two methods for light-based likelihood calculations. The simplest is using tag-based light levels to estimate sunrise and sunset times and thus position. This approach is performed by <code>calc.srss</code> but is an overly simplistic treatment of this data so we often 1) throw out latitude estimates and only keep longitude and 2) get a lot of bad location information, particularly from species that don’t frequent the photic zone. A somewhat improved approach is to use the more complex light-based geolocation algorithm previously employed by the tag manufacturer, Wildlife Computers, called GPE2. This uses a threshold approach developed by <span class="citation">Hill and Braun (<a href="#ref-Hill2001">2001</a>)</span> which results in 1) less spurious position estimates and 2) user-controlled vetting process of estimates via a GUI. This functionality requires that the user process their light data using the GPE2 environment which is no longer supported by the manufacturer but is still available <a href="http://wildlifecomputers.com/support/downloads/">here</a>. GPE2 output data is used in <code>HMMoce</code> with <code>calc.gpe2</code>. In either case, the resulting <code>L.light</code> raster contains daily (when available) likelihood surfaces representing the likelihood of an animal’s location based on light data.</p>
</div>
<div id="sea-surface-temperature-likelihood" class="section level3">
<h3>Sea surface temperature likelihood</h3>
<p>The SST likelihood approach in <code>HMMoce</code> is currently very simple. We represent tag-based SST data as a daily SST measurement <span class="math inline">\(\pm\)</span> sensor error (currently fixed at a conservative 5%) and compare that to a remotely-sensed SST product. We currently use the <a href="https://www.ncdc.noaa.gov/oisst">Optimum Interpolation</a> product due to its relatively comprehensive coverage and 1/4<span class="math inline">\(^\circ\)</span> resolution; however, any SST product could be used here with only minor changes to the download function, <code>get.env</code>. The output from <code>calc.sst</code> is a raster of daily likelihood surfaces for the animal’s movements based on SST measurements.</p>
</div>
<div id="depth-temperature-profile-likelihood" class="section level3">
<h3>Depth-temperature profile likelihood</h3>
<p>The depth-temperature profile (PDT) data from the tag is the main contribution of <code>HMMoce</code> to the marine animal tracking community. This functionality allows users to use unique depth-temperature signatures measured by tagged animals to improve position estimates, which is particularly useful for study species that rarely visit the photic zone during the day (e.g. swordfish, <span class="citation">Neilson et al. (<a href="#ref-Neilson2009">2009</a>)</span>) or spend considerable periods of time in the mesopelagic (e.g. basking sharks, <span class="citation">Skomal et al. (<a href="#ref-Skomal2009">2009</a>)</span>). In <code>HMMoce</code>, there are currently two approaches to using the PDT data for geolocation. The first follows <span class="citation">Luo et al. (<a href="#ref-Luo2015">2015</a>)</span> by integrating profile data to calculate a metric called Ocean Heat Content (OHC). We integrate tag-based PDT data from a certain isotherm to the surface to calculate the “heat content” of that layer measured by the tagged animal. Similarly, we perform the same integration on the model ocean as represented in the HYbrid Coordinate Ocean Model (<a href="http://hycom.org/">HYCOM</a>) and compare the two integrated metrics to generate a likelihood (<code>L.ohc</code>) of the animal’s position. The second approach is to use the profile in 3D space and compare it to oceanography at measured depth levels. This uses the same tag-based PDT data (not integrated) and compares it to modeled HYCOM data or climatological mean data contained in the World Ocean Atlas (<a href="https://www.nodc.noaa.gov/OC5/woa13/">WOA</a>). In either case, we use a linear regression to predict the tag-based temperature at the standard depth levels measured in one of the oceanographic datasets. Then a likelihood is calculated in the same fashion by comparing temperature from the tag to ocean temperature at each depth level and resulting likelihood layers are multiplied across depth levels to result in a single daily likelihood layer based on the tagged animal’s dive data (<code>L.prof</code>).</p>
</div>
<div id="finishing-the-observations" class="section level3">
<h3>Finishing the observations</h3>
<p>After one or more of the above observation datasets are used to calculate their respective likelihood layers, we resample the resulting rasters to ensure comparable extent and resolution of the likelihood layers (<code>resample.grid</code>). Once the likelihood layers are all comparable, we perform the final step of the observation model using <code>make.L</code>. This function takes input likelihood layers from different data sources and combines them to form a single, overall likelihood for each day. Known locations can also be incorporated here if there are any sightings, acoustic tag, or other sources of additional information for this individual. The resulting <code>L</code> array from <code>make.L</code> is carried forward into the convolution of our observations with theory in the hidden Markov model.</p>
</div>
</div>
<div id="the-hidden-markov-model" class="section level2">
<h2>The hidden Markov model</h2>
<p>Now that the observation data has been used to generate daily likelihoods, we need a way to represent the most likely way the animal moved through these likelihood surfaces. To do this, we generate a theoretical movement model based simply on daily diffusion and 2 behavior states. That is, the animal is represented virtually as a particle that is allowed to diffuse in our virtual ocean based on 2 different behavior states, for example, migratory and resident behavior. Each behavior state is represented by different diffusion metrics in which we expect a migratory animal to diffuse (move) much more widely than a resident animal. The best approach to calculating the diffusive metrics are currently fixing them based on expert knowledge of the tagged animal. The other parameters governing the theoretical movement of our virtual animal is the probability of switching between the two behavior states. This is represented as a 2 x 2 matrix where [1,1] indicates the probability of an animal staying in state 1 given it’s currently in state 1 and [1,2] is the probability of the animal moving to state 2 given it’s in state 1. The same is true for [2,]. These parameters are calculated using an Expectation-Maximization algorithm similar to <span class="citation">Woillez et al. (<a href="#ref-Woillez2016">2016</a>)</span>. This is simply performed for you using <code>expmax</code> and an initial guess at these transition probabilities (default values are also supplied).</p>
<p>Once all the parameters are in order, the theoretical movement model and the observations are convolved in a HMM filter that provides the probability distribution of the states (location and behavior) forward in time conditional on data. These state estimates are calculated successively by alternating between so-called time and data updates of the current state. Following filtering, the recursions of the HMM smoothing step work backwards in time using the filtered state estimates and all available data to determine the smoothed state estimates. The smoothed state estimates are more accurate and generally appear ‘smoother’ than the filtering estimates because they exploit the full data set. The probability distribution of all states at specific times are the state estimates returned from the HMM smoothing algorithm. Finally, the fun begins by plotting your results! Follow the example below to see how this all fits together and to implement <code>HMMoce</code> for your own data.</p>
</div>
</div>
<div id="example-use-of-hmmoce" class="section level1">
<h1>Example use of <code>HMMoce</code></h1>
<div id="installation-and-setup" class="section level2">
<h2>Installation and Setup</h2>
<p>You can install <code>HMMoce</code> immediately from GitHub using the <code>devtools</code> package:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">devtools::install_git{<span class="st">'https://github.com/camrinbraun/HMMoce'</span>, depends =<span class="st"> </span>T}
<span class="co"># then load the package</span>
<span class="kw">library</span>(HMMoce)

<span class="co"># it will be available on CRAN eventually. get it there using:</span>
<span class="kw">install.packages</span>(<span class="st">'HMMoce'</span>)</code></pre></div>
</div>
<div id="the-observation-model-1" class="section level2">
<h2>The Observation Model</h2>
<div id="reading-and-formatting-tag-data" class="section level3">
<h3>Reading and Formatting Tag Data</h3>
<p>With the package installed, you’re ready to get started on your own data. We’ve included some example blue shark data with the package so you can see how it all works. Note that <code>HMMoce</code> v1.0 is only compatible with Wildlife Computers PSAT tags. We anticipate adding additional tag type/manufacturer functionality based on user needs. We assume the tag data source .csv files have been downloaded from the Wildlife Computers data portal. For more on the portal visit <a href="http://wildlifecomputers.com/">Wildlife Computers</a>.</p>
<p>Start by setting the tag and pop-up locations and dates. This can easily be cleaned up if you have a .csv file containing tag metadata, for example, but for now we do it by individual for simplicity.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## # PTT or Unique Individual ID
## ptt &lt;- 141259
## 
## # TAG/POPUP DATES AND LOCATIONS (dd, mm, YYYY, lat, lon)
## iniloc &lt;- data.frame(matrix(c(13, 10, 2015, 41.575, -69.423, 
##                               24, 2, 2016, 26.6798, -69.0147), nrow = 2, ncol = 5, byrow = T))
## colnames(iniloc) = list('day','month','year','lat','lon')
## tag &lt;- as.POSIXct(paste(iniloc[1,1], '/', iniloc[1,2], '/', iniloc[1,3], sep=''), format = '%d/%m/%Y')
## pop &lt;- as.POSIXct(paste(iniloc[2,1], '/', iniloc[2,2], '/', iniloc[2,3], sep=''), format = '%d/%m/%Y')
## 
## # VECTOR OF DATES FROM DATA. THIS WILL BE THE TIME STEPS, T, IN THE LIKELIHOODS
## dateVec &lt;- as.Date(seq(tag, pop, by = 'day'))</code></pre></div>
<p>Then set the directory where your data lives and load the necessary files. Here we choose to load all the available data for this tag to demonstrate how all the different types are leveraged later on. The data types are sea surface temperature (SST), depth-temperature profiles (PDT), light levels and a file called <code>gpe2</code> that contains output light data from the Wildlife Computers GPE2 software (see light section above).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># READ IN DATA FROM WC FILES</span>
myDir &lt;-<span class="st"> '~/Documents/WHOI/RCode/HMMoce/inst/extdata/'</span> <span class="co"># WHERE YOUR DATA LIVES, THIS IS THE EXAMPLE DATA</span>

<span class="co"># sst data</span>
tag.sst &lt;-<span class="st"> </span><span class="kw">read.wc</span>(ptt, <span class="dt">wd =</span> myDir, <span class="dt">type =</span> <span class="st">'sst'</span>, <span class="dt">tag=</span>tag, <span class="dt">pop=</span>pop); 
sst.udates &lt;-<span class="st"> </span>tag.sst$udates; tag.sst &lt;-<span class="st"> </span>tag.sst$data

<span class="co"># depth-temp profile data</span>
pdt &lt;-<span class="st"> </span><span class="kw">read.wc</span>(ptt, <span class="dt">wd =</span> myDir, <span class="dt">type =</span> <span class="st">'pdt'</span>, <span class="dt">tag=</span>tag, <span class="dt">pop=</span>pop); 
pdt.udates &lt;-<span class="st"> </span>pdt$udates; pdt &lt;-<span class="st"> </span>pdt$data

<span class="co"># light data</span>
light &lt;-<span class="st"> </span><span class="kw">read.wc</span>(ptt, <span class="dt">wd =</span> myDir, <span class="dt">type =</span> <span class="st">'light'</span>, <span class="dt">tag=</span>tag, <span class="dt">pop=</span>pop); 
light.udates &lt;-<span class="st"> </span>light$udates; light &lt;-<span class="st"> </span>light$data

<span class="co"># light data as output from GPE2, different filtering algorithm seems to work better for light likelihood generation</span>
gpe2 &lt;-<span class="st"> </span><span class="kw">read.table</span>(<span class="kw">paste</span>(ptt, <span class="st">'-Locations-GPE2.csv'</span>, <span class="dt">sep =</span> <span class="st">''</span>), <span class="dt">sep =</span> <span class="st">','</span>, <span class="dt">header =</span> T, <span class="dt">blank.lines.skip =</span> F)</code></pre></div>
<p>The <code>read.wc</code> function reads the type of data requested and automatically formats it for use in the likelihood calculations. See <code>?read.wc</code> for more information and a list of available data input types.</p>
<p>The final formatting step involves setting spatial boundaries to work within. This can either be set manually or by passing a -Locations.csv file such as that loaded above.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># SET SPATIAL LIMITS, IF DESIRED, OR PASS GPE FILE</span>
sp.lim &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="dt">lonmin =</span> -<span class="dv">82</span>, <span class="dt">lonmax =</span> -<span class="dv">25</span>, <span class="dt">latmin =</span> <span class="dv">15</span>, <span class="dt">latmax =</span> <span class="dv">50</span>)

if (<span class="kw">exists</span>(<span class="st">'sp.lim'</span>)){
  locs.grid &lt;-<span class="st"> </span><span class="kw">setup.locs.grid</span>(sp.lim)
} else{
  locs.grid &lt;-<span class="st"> </span><span class="kw">setup.locs.grid</span>(gpe2)
  sp.lim &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="dt">lonmin =</span> <span class="kw">min</span>(locs.grid$lon[<span class="dv">1</span>,]), <span class="dt">lonmax =</span> <span class="kw">max</span>(locs.grid$lon[<span class="dv">1</span>,]),
                 <span class="dt">latmin =</span> <span class="kw">min</span>(locs.grid$lat[,<span class="dv">1</span>]), <span class="dt">latmax =</span> <span class="kw">max</span>(locs.grid$lat[,<span class="dv">1</span>]))
}</code></pre></div>
</div>
<div id="getting-environmental-data" class="section level3">
<h3>Getting Environmental Data</h3>
<p>Before we can calculate likelihoods, we need to have the environmental data to compare our tag measurements to. To do this, <code>HMMoce</code> has a <code>get.env</code> function for which you specify your dates of interest, spatial limits, and type of data you need. The <code>get.env</code> function then goes online and downloads the requested data. The SST default data is the Optimally Interpolated (OI) 1/4<span class="math inline">\(^\circ\)</span> <a href="https://www.ncdc.noaa.gov/oisst">product</a> but additional datasets can easily be added based on user input. Depth-temperature profiles are compared to <a href="http://hycom.org/dataserver/glb-analysis">HYCOM</a> predictions from the 1/12<span class="math inline">\(^\circ\)</span> global analysis or the <a href="https://www.nodc.noaa.gov/OC5/woa13/">WOA</a> climatology. The HYCOM datasets for the duration of a tag deployment can be large so we recommend running the HYCOM download overnight. If you plan to analyze multiple tag datasets over a similar time period and spatial domain, consider that before downloading the environmental datasets to save yourself from having to change spatial/temporal bounds later and download everything again! The WOA data is from the link above but is hosted in a more accessible form for our purposes on Github (use <code>get.env(type = 'woa')</code> with your desired resolution argument).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># IF YOU NEED TO DOWNLOAD SST DATA</span>
sst.dir &lt;-<span class="st"> </span><span class="kw">paste</span>(<span class="st">'my_sst_dir'</span>)
<span class="kw">get.env</span>(sst.udates, <span class="dt">type =</span> <span class="st">'sst'</span>, <span class="dt">spatLim =</span> sp.lim, <span class="dt">save.dir =</span> sst.dir)

<span class="co"># HYCOM DATA</span>
hycom.dir &lt;-<span class="st"> </span><span class="kw">paste</span>(<span class="st">'my_hycom_dir'</span>)
<span class="kw">get.env</span>(pdt.udates, <span class="dt">type =</span> <span class="st">'hycom'</span>, <span class="dt">spatLim =</span> sp.lim, <span class="dt">save.dir =</span> hycom.dir)

<span class="co"># AND/OR WOA DATA</span>
woa.dir &lt;-<span class="st"> </span><span class="kw">paste</span>(<span class="st">'my_woa_dir'</span>)
<span class="kw">get.env</span>(<span class="dt">type =</span> <span class="st">'woa'</span>, <span class="dt">resol =</span> <span class="st">'quarter'</span>)</code></pre></div>
</div>
<div id="generating-likelihoods" class="section level3">
<h3>Generating Likelihoods</h3>
<p>Once the environmental data is downloaded to its respective directory, you’re ready for likelihood calculations. Each data type has its own likelihood function that does the grunt work for you. Note that due to the nature of complex (and high resolution) 3D data, particularly in <code>calc.profile(envType = 'hycom')</code>, these calculations can take as long as a few hours for a 6 month, basin-scale likelihood calculation. The light and SST calculations, on the other hand, take a matter of seconds.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># GENERATE LIGHT LIKELIHOOD</span>
<span class="co"># SRSS METHOD</span>
L.light &lt;-<span class="st"> </span><span class="kw">calc.srss</span>(light, <span class="dt">locs.grid =</span> locs.grid, <span class="dt">dateVec =</span> dateVec)
<span class="co"># OR</span>
<span class="co"># GPE2 METHOD</span>
L.light &lt;-<span class="st"> </span><span class="kw">calc.gpe2</span>(locs, <span class="dt">iniloc =</span> iniloc, <span class="dt">locs.grid =</span> locs.grid, <span class="dt">dateVec =</span> dateVec, <span class="dt">errEll =</span> <span class="ot">TRUE</span>, <span class="dt">gpeOnly =</span> <span class="ot">TRUE</span>)

<span class="co">#-------</span>
<span class="co"># GENERATE DAILY SST LIKELIHOODS</span>
L.sst &lt;-<span class="st"> </span><span class="kw">calc.sst</span>(tag.sst, <span class="dt">sst.dir =</span> sst.dir, <span class="dt">dateVec =</span> dateVec)

<span class="co">#-------</span>
<span class="co"># GENERATE DAILY OCEAN HEAT CONTENT (OHC) LIKELIHOODS</span>
L.ohc &lt;-<span class="st"> </span><span class="kw">calc.ohc</span>(pdt, ptt, <span class="dt">ohc.dir =</span> hycom.dir, <span class="dt">dateVec =</span> dateVec, <span class="dt">isotherm =</span> <span class="st">''</span>)

<span class="co">#-------</span>
<span class="co"># GENERATE DAILY PROFILE LIKELIHOODS</span>
L.prof &lt;-<span class="st"> </span><span class="kw">calc.profile</span>(pdt, ptt, <span class="dt">dateVec =</span> dateVec, <span class="dt">envType =</span> <span class="st">'hycom'</span>, <span class="dt">hycom.dir =</span> hycom.dir)
<span class="co">#L.prof.woa &lt;- calc.profile(pdt, dat = woa, lat = lat, lon = lon, dateVec = dateVec, envType = 'woa')</span></code></pre></div>
</div>
<div id="resampling-and-combining-likelihoods" class="section level3">
<h3>Resampling and Combining Likelihoods</h3>
<p>After your desired likelihood calculations have been made, we need to do some housekeeping. We currently have several likelihood rasters from different data sources that need to be combined to form overall likelihoods for each time point (usually one day). To do this, we need to resample all the likelihood rasters to the same extent and resolution using <code>resample.grid</code>. This function resamples the input likelihood rasters so they all match the extent and resolution of one of the inputs. It also returns a variable called <code>L.mle.res</code> which is typically a more coarse (lower resolution) representation of the overall likelihoods to speed up parameter estimation in the next step. After resampling, we simply use <code>make.L</code> to construct our overall likelihood. This part of the process looks something like:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co">#----------------------------------------------------------------------------------#</span>
<span class="co"># SETUP A COMMON GRID</span>
<span class="co">#----------------------------------------------------------------------------------#</span>
<span class="co"># create a list of all the desired input likelihood rasters</span>
L.rasters &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="dt">L.sst =</span> L.sst, <span class="dt">L.light =</span> L.light)
<span class="co"># L.sst is the resolution/extent we're sampling everything TO</span>
L.res &lt;-<span class="st"> </span><span class="kw">resample.grid</span>(L.rasters, L.rasters$L.sst)

<span class="co"># pull some other helpful variables from the resample.grid() output for later use</span>
L.mle.res &lt;-<span class="st"> </span>L.res$L.mle.res
g &lt;-<span class="st"> </span>L.res$g; lon &lt;-<span class="st"> </span>g$lon[<span class="dv">1</span>,]; lat &lt;-<span class="st"> </span>g$lat[,<span class="dv">1</span>]
g.mle &lt;-<span class="st"> </span>L.res$g.mle

<span class="co">#----------------------------------------------------------------------------------#</span>
<span class="co"># LOAD AND FORMAT DATAFRAME OF KNOWN LOCATIONS, IF ANY</span>
<span class="co">#----------------------------------------------------------------------------------#</span>

<span class="co">#colnames(known.locs) &lt;- list('date','lat','lon')</span>
<span class="co">#   where 'date' is from as.Date(known.locs$date)</span>

<span class="co">#----------------------------------------------------------------------------------#</span>
<span class="co"># COMBINE LIKELIHOOD MATRICES</span>
<span class="co">#----------------------------------------------------------------------------------#</span>
<span class="co"># this example just uses L.sst and L.light. You can list up to three (L1, L2, L3 inputs).</span>
L &lt;-<span class="st"> </span><span class="kw">make.L</span>(<span class="dt">L1 =</span> L.res[[<span class="dv">1</span>]]$L.sst,
            <span class="dt">L2 =</span> L.res[[<span class="dv">1</span>]]$L.light,
            <span class="dt">L.mle.res =</span> L.mle.res, <span class="dt">dateVec =</span> dateVec,
            <span class="dt">locs.grid =</span> locs.grid, <span class="dt">iniloc =</span> iniloc)

L.mle &lt;-<span class="st"> </span>L$L.mle; L &lt;-<span class="st"> </span>L$L</code></pre></div>
<p>Once you’ve made it this far, the observation model component is complete! The output array, <code>L</code>, contains the observation-based likelihood surfaces for each day of tag deployment. Next we convolve the observation data with theoretical movements in a hidden Markov framework.</p>
</div>
</div>
<div id="the-theory-and-hmm" class="section level2">
<h2>The Theory and HMM</h2>
<div id="parameter-estimation" class="section level3">
<h3>Parameter Estimation</h3>
<p>We now need an idea of how our tagged animal would theoretically move assuming purely diffusive (Brownian) motion. Because we are assuming 2 behavior states, we need to parameterize diffusive movement for each of those states. Let’s assume state 1 is a migratory movement and state 2 is resident behavior. We provide fixed characteristics of diffusion using kernels.</p>
<p>WE NEED TO FIGURE OUT AN EASY WAY FOR USERS TO FIX THEIR OWN PARAMETERS.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># PROVIDE FIXED KERNEL PARAMETERS</span>
par0 &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="fl">8.908</span>,<span class="fl">10.27</span>,<span class="fl">1.152</span>,<span class="fl">0.0472</span>)
D1 &lt;-<span class="st"> </span>par0[<span class="dv">1</span>:<span class="dv">2</span>] <span class="co"># parameters for kernel 1. this is migratory behavior mode</span>
D2 &lt;-<span class="st"> </span>par0[<span class="dv">3</span>:<span class="dv">4</span>] <span class="co"># parameters for kernel 2. resident behavior mode</span>

<span class="co"># GENERATE MOVEMENT KERNELS. D VALUES ARE MEAN AND SD PIXELS</span>
K1 &lt;-<span class="st"> </span><span class="kw">gausskern</span>(D1[<span class="dv">1</span>], D1[<span class="dv">2</span>], <span class="dt">muadv =</span> <span class="dv">0</span>)
K2 &lt;-<span class="st"> </span><span class="kw">gausskern</span>(D2[<span class="dv">1</span>], D2[<span class="dv">2</span>], <span class="dt">muadv =</span> <span class="dv">0</span>)</code></pre></div>
<p>We also need the probability of switching (or remaining in) each of our states. If we make an initial guess, we can use Expectation-Maximization to calculate our switch parameters. If <code>expmax</code> is running slowly, change <code>g</code> and <code>L</code> arguments to <code>g.mle</code> and <code>L.mle</code> which are more coarse versions of our likelihood and corresponding grid.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># MAKE A GUESS AT STATE SWITCHING PROBABILITY</span>
<span class="co"># probability of staying in state 1 and 2, respectively</span>
p.init &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="fl">0.7</span>, <span class="fl">0.8</span>)

<span class="co"># RUN EXPECTATION-MAXIMIZATION ROUTINE FOR MATRIX, P (STATE SWITCH PROBABILITY)</span>
P.final &lt;-<span class="st"> </span><span class="kw">expmax</span>(p.init, <span class="dt">g =</span> g, <span class="dt">L =</span> L, K1, K2)</code></pre></div>
<p>Now that we have our movement kernels and our switching probabilities figured out, it’s time for the HMM.</p>
</div>
<div id="the-hmm" class="section level3">
<h3>The HMM</h3>
<p>First, a filtering routine provides the probability distribution of the unknown states (location and behavior) forward in time conditional on data (the observation model). These state estimates are calculated successively by alternating between so-called time and data updates of the current state. Following filtering, the recursions of the HMM smoothing step work backwards in time using the filtered state estimates and all available data to determine the smoothed state estimates. The smoothed state estimates are equivalent to the posterior distribution of the state (at each time) which can be used for calculating the most probable track, utilization distributions and behavior metrics.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co">#----------------------------------------------------------------------------------#</span>
<span class="co"># RUN THE FILTER STEP</span>
f &lt;-<span class="st"> </span><span class="kw">hmm.filter</span>(g, L, K1, K2, P.final)

<span class="co">#----------------------------------------------------------------------------------#</span>
<span class="co"># RUN THE SMOOTHING STEP</span>
s =<span class="st"> </span><span class="kw">hmm.smoother</span>(f, K1, K2, P.final)</code></pre></div>
</div>
<div id="most-probable-track-and-plotting" class="section level3">
<h3>Most Probable Track and Plotting</h3>
<p>There are a number of ways we can use the posterior distribution of the state to estimate animal movements. The two most simple methods are to calculate the mean or mode value (lat/lon) across the distributions at each time. These are performed using <code>calc.track</code>. Finally, use <code>plotHMM</code> for visualizing the final track and state estimates.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co">#----------------------------------------------------------------------------------#</span>
<span class="co"># GET AND PLOT THE MOST PROBABLE TRACK</span>
<span class="co">#----------------------------------------------------------------------------------#</span>

tr &lt;-<span class="st"> </span><span class="kw">calc.track</span>(s, g, dateVec)

<span class="kw">plotHMM</span>(s, tr, dateVec, <span class="dt">save.plot =</span> <span class="ot">TRUE</span>)</code></pre></div>
</div>
</div>
<div id="a-full-example-script" class="section level2">
<h2>A Full Example Script</h2>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># RUN EXAMPLE BLUE SHARK (141259) VIA HMMoce</span>
<span class="kw">library</span>(HMMoce)

<span class="co"># TAG/POPUP DATES AND LOCATIONS (dd, mm, YYYY, lat, lon)</span>
iniloc &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="kw">matrix</span>(<span class="kw">c</span>(<span class="dv">13</span>, <span class="dv">10</span>, <span class="dv">2015</span>, <span class="fl">41.3</span>, -<span class="fl">69.27</span>, 
                              <span class="dv">10</span>, <span class="dv">4</span>, <span class="dv">2016</span>, <span class="fl">40.251</span>, -<span class="fl">36.061</span>), <span class="dt">nrow =</span> <span class="dv">2</span>, <span class="dt">ncol =</span> <span class="dv">5</span>, <span class="dt">byrow =</span> T))
<span class="kw">colnames</span>(iniloc) =<span class="st"> </span><span class="kw">list</span>(<span class="st">'day'</span>,<span class="st">'month'</span>,<span class="st">'year'</span>,<span class="st">'lat'</span>,<span class="st">'lon'</span>)
tag &lt;-<span class="st"> </span><span class="kw">as.POSIXct</span>(<span class="kw">paste</span>(iniloc[<span class="dv">1</span>,<span class="dv">1</span>], <span class="st">'/'</span>, iniloc[<span class="dv">1</span>,<span class="dv">2</span>], <span class="st">'/'</span>, iniloc[<span class="dv">1</span>,<span class="dv">3</span>], <span class="dt">sep=</span><span class="st">''</span>), <span class="dt">format =</span> <span class="st">'%d/%m/%Y'</span>, <span class="dt">tz=</span><span class="st">'UTC'</span>)
pop &lt;-<span class="st"> </span><span class="kw">as.POSIXct</span>(<span class="kw">paste</span>(iniloc[<span class="dv">2</span>,<span class="dv">1</span>], <span class="st">'/'</span>, iniloc[<span class="dv">2</span>,<span class="dv">2</span>], <span class="st">'/'</span>, iniloc[<span class="dv">2</span>,<span class="dv">3</span>], <span class="dt">sep=</span><span class="st">''</span>), <span class="dt">format =</span> <span class="st">'%d/%m/%Y'</span>, <span class="dt">tz=</span><span class="st">'UTC'</span>)

<span class="co"># VECTOR OF DATES FROM DATA. THIS WILL BE THE TIME STEPS, T, IN THE LIKELIHOODS</span>
dateVec &lt;-<span class="st"> </span><span class="kw">as.Date</span>(<span class="kw">seq</span>(tag, pop, <span class="dt">by =</span> <span class="st">'day'</span>)) 

<span class="co"># READ IN DATA FROM WC FILES</span>
<span class="co">#myDir &lt;- '../HMMoce/inst/extdata/' # WHERE YOUR DATA LIVES, THIS IS THE EXAMPLE DATA</span>

<span class="co"># PTT or unique ID</span>
ptt &lt;-<span class="st"> </span><span class="dv">141259</span>

<span class="co"># sst data</span>
tag.sst &lt;-<span class="st"> </span><span class="kw">read.wc</span>(ptt, <span class="dt">wd =</span> myDir, <span class="dt">type =</span> <span class="st">'sst'</span>, <span class="dt">tag=</span>tag, <span class="dt">pop=</span>pop); 
sst.udates &lt;-<span class="st"> </span>tag.sst$udates; tag.sst &lt;-<span class="st"> </span>tag.sst$data

<span class="co"># depth-temp profile data</span>
pdt &lt;-<span class="st"> </span><span class="kw">read.wc</span>(ptt, <span class="dt">wd =</span> myDir, <span class="dt">type =</span> <span class="st">'pdt'</span>, <span class="dt">tag=</span>tag, <span class="dt">pop=</span>pop); 
pdt.udates &lt;-<span class="st"> </span>pdt$udates; pdt &lt;-<span class="st"> </span>pdt$data

<span class="co"># light data</span>
light &lt;-<span class="st"> </span><span class="kw">read.wc</span>(ptt, <span class="dt">wd =</span> myDir, <span class="dt">type =</span> <span class="st">'light'</span>, <span class="dt">tag=</span>tag, <span class="dt">pop=</span>pop); 
light.udates &lt;-<span class="st"> </span>light$udates; light &lt;-<span class="st"> </span>light$data

<span class="co"># light data as output from GPE2, different filtering algorithm seems to work better for light likelihood generation</span>
locs &lt;-<span class="st"> </span><span class="kw">read.table</span>(<span class="kw">paste</span>(ptt, <span class="st">'-Locations-GPE2.csv'</span>, <span class="dt">sep =</span> <span class="st">''</span>), <span class="dt">sep =</span> <span class="st">','</span>, <span class="dt">header =</span> T, <span class="dt">blank.lines.skip =</span> F)

<span class="co">#----------------------------------------------------------------------------------#</span>
<span class="co"># FURTHER PREPARATION</span>
<span class="co"># Set spatial limits and download env data</span>
<span class="co">#----------------------------------------------------------------------------------#</span>

<span class="co"># SET SPATIAL LIMITS, IF DESIRED</span>
sp.lim &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="dt">lonmin =</span> -<span class="dv">82</span>, <span class="dt">lonmax =</span> -<span class="dv">25</span>, <span class="dt">latmin =</span> <span class="dv">15</span>, <span class="dt">latmax =</span> <span class="dv">50</span>)

if (<span class="kw">exists</span>(<span class="st">'sp.lim'</span>)){
  locs.grid &lt;-<span class="st"> </span><span class="kw">setup.locs.grid</span>(sp.lim)
} else{
  locs.grid &lt;-<span class="st"> </span><span class="kw">setup.locs.grid</span>(locs)
  sp.lim &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="dt">lonmin =</span> <span class="kw">min</span>(locs.grid$lon[<span class="dv">1</span>,]), <span class="dt">lonmax =</span> <span class="kw">max</span>(locs.grid$lon[<span class="dv">1</span>,]),
                 <span class="dt">latmin =</span> <span class="kw">min</span>(locs.grid$lat[,<span class="dv">1</span>]), <span class="dt">latmax =</span> <span class="kw">max</span>(locs.grid$lat[,<span class="dv">1</span>]))
}

<span class="co"># IF YOU NEED TO DOWNLOAD SST DATA</span>
sst.dir &lt;-<span class="st"> </span><span class="kw">paste</span>(<span class="st">'my_sst_dir'</span>)
<span class="kw">get.env</span>(sst.udates, <span class="dt">type =</span> <span class="st">'sst'</span>, <span class="dt">spatLim =</span> sp.lim, <span class="dt">save.dir =</span> sst.dir)

<span class="co"># HYCOM DATA</span>
hycom.dir &lt;-<span class="st"> </span><span class="kw">paste</span>(<span class="st">'my_hycom_dir'</span>)
<span class="kw">get.env</span>(pdt.udates, <span class="dt">type =</span> <span class="st">'hycom'</span>, <span class="dt">spatLim =</span> sp.lim, <span class="dt">save.dir =</span> hycom.dir)

<span class="co"># AND/OR WOA DATA</span>
woa.dir &lt;-<span class="st"> </span><span class="kw">paste</span>(<span class="st">'my_woa_dir'</span>)
<span class="kw">get.env</span>(<span class="dt">type =</span> <span class="st">'woa'</span>, <span class="dt">resol =</span> <span class="st">'quarter'</span>)

<span class="co">#----------------------------------------------------------------------------------#</span>
<span class="co"># CALC LIKELIHOODS</span>
<span class="co">#----------------------------------------------------------------------------------#</span>

<span class="co"># GENERATE LIGHT LIKELIHOOD</span>
<span class="co"># SRSS METHOD</span>
L.light &lt;-<span class="st"> </span><span class="kw">calc.srss</span>(light, <span class="dt">locs.grid =</span> locs.grid, <span class="dt">dateVec =</span> dateVec)
<span class="co"># OR</span>
<span class="co"># GPE2 METHOD</span>
L.light &lt;-<span class="st"> </span><span class="kw">calc.gpe2</span>(locs, <span class="dt">iniloc =</span> iniloc, <span class="dt">locs.grid =</span> locs.grid, <span class="dt">dateVec =</span> dateVec, <span class="dt">errEll =</span> <span class="ot">TRUE</span>, <span class="dt">gpeOnly =</span> <span class="ot">TRUE</span>)


<span class="co"># GENERATE DAILY SST LIKELIHOODS</span>
L.sst &lt;-<span class="st"> </span><span class="kw">calc.sst</span>(tag.sst, <span class="dt">sst.dir =</span> sst.dir, <span class="dt">dateVec =</span> dateVec)

<span class="co">#-------</span>
<span class="co"># GENERATE DAILY OCEAN HEAT CONTENT (OHC) LIKELIHOODS</span>
L.ohc &lt;-<span class="st"> </span><span class="kw">calc.ohc</span>(pdt, ptt, <span class="dt">ohc.dir =</span> hycom.dir, <span class="dt">dateVec =</span> dateVec, <span class="dt">isotherm =</span> <span class="st">''</span>)

<span class="co">#-------</span>
<span class="co"># GENERATE DAILY PROFILE LIKELIHOODS</span>
L.prof &lt;-<span class="st"> </span><span class="kw">calc.profile</span>(pdt, ptt, <span class="dt">dateVec =</span> dateVec, <span class="dt">envType =</span> <span class="st">'hycom'</span>, <span class="dt">hycom.dir =</span> hycom.dir)
<span class="co">#L.prof.woa &lt;- calc.profile(pdt, dat = woa, lat = lat, lon = lon, dateVec = dateVec, envType = 'woa')</span>

<span class="co">#----------------------------------------------------------------------------------#</span>
<span class="co"># SETUP A COMMON GRID</span>
<span class="co">#----------------------------------------------------------------------------------#</span>
<span class="co"># create a list of all the desired input likelihood rasters</span>
L.rasters &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="dt">L.sst =</span> L.sst, <span class="dt">L.light =</span> L.light)
<span class="co"># L.sst is the resolution/extent we're sampling everything TO</span>
L.res &lt;-<span class="st"> </span><span class="kw">resample.grid</span>(L.rasters, L.rasters$L.sst)

<span class="co"># pull some other helpful variables from the resample.grid() output for later use</span>
L.mle.res &lt;-<span class="st"> </span>L.res$L.mle.res
g &lt;-<span class="st"> </span>L.res$g; lon &lt;-<span class="st"> </span>g$lon[<span class="dv">1</span>,]; lat &lt;-<span class="st"> </span>g$lat[,<span class="dv">1</span>]
g.mle &lt;-<span class="st"> </span>L.res$g.mle

<span class="co">#----------------------------------------------------------------------------------#</span>
<span class="co"># LOAD AND FORMAT DATAFRAME OF KNOWN LOCATIONS, IF ANY</span>
<span class="co">#----------------------------------------------------------------------------------#</span>

<span class="co">#colnames(known.locs) &lt;- list('date','lat','lon')</span>
<span class="co">#   where 'date' is from as.Date(known.locs$date)</span>

<span class="co">#----------------------------------------------------------------------------------#</span>
<span class="co"># COMBINE LIKELIHOOD MATRICES</span>
<span class="co">#----------------------------------------------------------------------------------#</span>
<span class="co"># this example just uses L.sst and L.light. You can list up to three (L1, L2, L3 inputs).</span>
L &lt;-<span class="st"> </span><span class="kw">make.L</span>(<span class="dt">L1 =</span> L.res[[<span class="dv">1</span>]]$L.sst,
            <span class="dt">L2 =</span> L.res[[<span class="dv">1</span>]]$L.light,
            <span class="dt">L.mle.res =</span> L.mle.res, <span class="dt">dateVec =</span> dateVec,
            <span class="dt">locs.grid =</span> locs.grid, <span class="dt">iniloc =</span> iniloc)

L.mle &lt;-<span class="st"> </span>L$L.mle; L &lt;-<span class="st"> </span>L$L

<span class="co">#----------------------------------------------------------------------------------#</span>
<span class="co"># FIGURE OUT MOVEMENT PARAMETERS</span>
<span class="co">#----------------------------------------------------------------------------------#</span>

<span class="co"># PROVIDE FIXED KERNEL PARAMETERS</span>
par0 &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="fl">8.908</span>,<span class="fl">10.27</span>,<span class="fl">1.152</span>,<span class="fl">0.0472</span>)
D1 &lt;-<span class="st"> </span>par0[<span class="dv">1</span>:<span class="dv">2</span>] <span class="co"># parameters for kernel 1. this is migratory behavior mode</span>
D2 &lt;-<span class="st"> </span>par0[<span class="dv">3</span>:<span class="dv">4</span>] <span class="co"># parameters for kernel 2. resident behavior mode</span>

<span class="co"># GENERATE MOVEMENT KERNELS. D VALUES ARE MEAN AND SD PIXELS</span>
K1 &lt;-<span class="st"> </span><span class="kw">gausskern</span>(D1[<span class="dv">1</span>], D1[<span class="dv">2</span>], <span class="dt">muadv =</span> <span class="dv">0</span>)
K2 &lt;-<span class="st"> </span><span class="kw">gausskern</span>(D2[<span class="dv">1</span>], D2[<span class="dv">2</span>], <span class="dt">muadv =</span> <span class="dv">0</span>)

<span class="co"># MAKE A GUESS AT STATE SWITCHING PROBABILITY</span>
<span class="co"># probability of staying in state 1 and 2, respectively</span>
p.init &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="fl">0.7</span>, <span class="fl">0.8</span>)

<span class="co"># RUN EXPECTATION-MAXIMIZATION ROUTINE FOR MATRIX, P (STATE SWITCH PROBABILITY)</span>
P.final &lt;-<span class="st"> </span><span class="kw">expmax</span>(p.init, <span class="dt">g =</span> g, <span class="dt">L =</span> L, K1, K2)

<span class="co">#----------------------------------------------------------------------------------#</span>
<span class="co"># RUN THE FILTER STEP</span>
f &lt;-<span class="st"> </span><span class="kw">hmm.filter</span>(g, L, K1, K2, P.final)

<span class="co"># plot if you want to see confidence limits</span>
<span class="co">#res = apply(f$phi[1,,,],2:3,sum, na.rm=T)</span>
<span class="co">#fields::image.plot(lon, lat, res/max(res), zlim = c(.05,1))</span>

<span class="co">#----------------------------------------------------------------------------------#</span>
<span class="co"># RUN THE SMOOTHING STEP</span>
s =<span class="st"> </span><span class="kw">hmm.smoother</span>(f, K1, K2, P.final)

<span class="co"># plot if you want to see confidence limits</span>
<span class="co">#sres = apply(s[1,,,], 2:3, sum, na.rm=T)</span>
<span class="co">#fields::image.plot(lon, lat, sres/max(sres), zlim = c(.05,1))</span>

<span class="co">#----------------------------------------------------------------------------------#</span>
<span class="co"># GET AND PLOT THE MOST PROBABLE TRACK</span>
<span class="co">#----------------------------------------------------------------------------------#</span>

tr &lt;-<span class="st"> </span><span class="kw">calc.track</span>(s, g, dateVec)

<span class="kw">plotHMM</span>(s, tr, dateVec, <span class="dt">save.plot =</span> <span class="ot">TRUE</span>)

<span class="co">#=======================================================================================#</span>
<span class="co"># END</span>
<span class="co">#=======================================================================================#</span></code></pre></div>
</div>
</div>
<div id="references" class="section level1 unnumbered">
<h1>References</h1>
<div id="refs" class="references">
<div id="ref-Hill2001">
<p>Hill, Roger D, and Melinda J Braun. 2001. “Geolocation by light level.” In <em>Electronic Tagging and Tracking in Marine Fisheries: Proceedings of the Symposium on Tagging and Tracking Marine Fish with Electronic Devices</em>, 315–30. University of Hawaii: Springer.</p>
</div>
<div id="ref-Luo2015">
<p>Luo, Jiangang, Jerald S. Ault, Lynn K. Shay, John P. Hoolihan, Eric D. Prince, Craig a. Brown, and Jay R. Rooker. 2015. “Ocean Heat Content Reveals Secrets of Fish Migrations.” <em>Plos One</em> 10 (10): e0141101. doi:<a href="https://doi.org/10.1371/journal.pone.0141101">10.1371/journal.pone.0141101</a>.</p>
</div>
<div id="ref-Neilson2009">
<p>Neilson, John D, Sean Smith, François Royer, Stacey D Paul, Julie M Porter, and Molly Lutcavage. 2009. “Investigations of horizontal movements of Atlantic swordfish using pop-up satellite archival tags.” In <em>Tagging and Tracking of Marine Animals with Electronic Devices</em>, 145–59. Springer.</p>
</div>
<div id="ref-Skomal2009">
<p>Skomal, G B, S I Zeeman, J H Chisholm, E L Summers, H J Walsh, K W McMahon, and S R Thorrold. 2009. “Transequatorial migrations by basking sharks in the western Atlantic Ocean.” <em>Current Biology</em> 19 (12): 1019–22. doi:<a href="https://doi.org/10.1016/j.cub.2009.04.019">10.1016/j.cub.2009.04.019</a>.</p>
</div>
<div id="ref-Woillez2016">
<p>Woillez, Mathieu, Ronan Fablet, Tran Thanh Ngo, Maxime Lalire, Pascal Lazure, and Hélène de Pontual. 2016. “A HMM-based model to geolocate pelagic fish from high-resolution individual temperature and depth histories: European sea bass as a case study.” <em>Ecological Modelling</em> 321. Elsevier B.V.: 10–22. doi:<a href="https://doi.org/10.1016/j.ecolmodel.2015.10.024">10.1016/j.ecolmodel.2015.10.024</a>.</p>
</div>
</div>
</div>



<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
