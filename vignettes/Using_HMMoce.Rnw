\documentclass{article}

% \VignetteIndexEntry{Guide to using moveHMM}
% \VignetteEngine{knitr::knitr}


\usepackage[utf8]{inputenc}
\usepackage[bf,font={small,sl}]{caption} % for pretty captions
\usepackage{natbib} % for the bibliography
\usepackage{amsmath} % for align, cases...
\usepackage{amsfonts} % for mathbb...
\usepackage{listings} % for lstlisting
\usepackage{color} % for lstlisting background color
\usepackage{booktabs} % pretty tables
\usepackage{url}
\usepackage[margin=1in]{geometry}
\usepackage{natbib}

\title{\textbf{\texttt{HMMoce}: An R package for improved geolocation of archival‚Äêtagged fishes using a hidden Markov method}}
\author{Camrin Braun, Ben Galuardi, Paul Gatti \& Simon Thorrold}

\begin{document}
\maketitle

<<init, echo=FALSE, message=FALSE>>=
library(sp)
#library(HMMoce)
devtools::load_all('../HMMoce')
library(raster)
library(fields)
library(tidyverse)
options(tidy=TRUE)
knitr::opts_chunk$set(
  comment = '', fig.width = 6, fig.height = 6, tidy = TRUE)
dir <- getwd()
@

\tableofcontents


\section{Summary}
While the number of marine animals being tagged and tracked continues to grow, current satellite tracking techniques largely constrain meaninful inference to largescale movements of surface-dwelling species and are inherently prone to significant error. Hidden Markov models (HMMs) have become increasingly common in the analysis of animal movement data by incorporating underlying behavioral states into movement data. This discretized approach also provides efficient handling of grid-based oceanographic data and likelihood surfaces generated within the package. We present an open-source \texttt{R} package, \texttt{HMMoce}, that uses a state-space HMM approach to improve position estimates derived from electronic tags using three-dimensional oceanographic data. We demonstrate \texttt{HMMoce} with example blue shark (*Prionace glauca*) data that is included in the package. Our findings illustrate how our software leverages all available tag data, along with oceanographic information, to improve position estimates of tagged marine species. For more details on these methods, including thorough references to the literature \texttt{HMMoce} is based on, please refer to \citep{Braun2018a}.

\section{Introduction}

There are many approaches to estimating animal movements from various types of tag data. The paradigm in fish tracking has been to use light levels to estimate position, but many species spend considerable time away from the photic zone. Diving behavior, like a typical diel vertical migration exhibited by deep diving swordfish, can render light geolocation useless. Yet, deep diving provides depth-temperature profile data recorded by the archival tag as it samples throughout a tagged individual's vertical movements. This sampling provides a unique signature through the oceanographic environment that can be leveraged to help constrain position. When combined with other tag-measured data streams like sea surface temperature (SST), light levels and maximum diving depth, we expect a unique combination of oceanographic characteristics to be diagnostic of an animal's location. Thus, \texttt{HMMoce} seeks to provide the framework for improving estimates of animal movements based on these oceanographic characteristics and strives to automate much of the data formatting and calculations in a transparent and flexible way.



%The analysis of animal movement data has become increasingly important in terrestrial and marine ecology. Improvements in telemetry technology have resulted in an explosion in the volume of high precision data being collected. As a result, there are two challenges which researchers collecting these data regularly face: (1) data volume and (2) employing statistical methods which can accommodate some of the specific features of movement data \citep{patterson2009}.

%A substantial part of the literature on statistical modelling of animal movement data has focused on the intuitive approach of decomposing movement time series into distinct behavioural modes (a.k.a.\ bouts, states), via the use of so-called state-switching models. These approaches typically involve assuming movements of animals to be driven by stochastically evolving states, such as a slow moving state, which may be indicative of resting or foraging, versus faster movement states which might indicate transits between foraging patches. Associated with changes in movement speeds are changes to the distribution of directional changes in the movement (known as the turning angle --- see further description below).

%Bayesian methods which employ MCMC approaches have become very popular tools for the analysis of movement data using state-switching models \citep[e.g.][]{jonsen2005, morales2004}. Typically, these have been implemented using WinBUGS \citep[although see][]{mcclintock2012}. While these models are relatively straightforward to build and hence fit in WinBUGS, the estimation can be painfully slow due to slow mixing of the MCMC samplers.

%However, for an important subset of movement data, namely highly accurate position data (e.g.\ from GPS) --- and more generally all time series of locations where the measurement error is negligible relative to the scale of the movement --- the task of statistical classification of behaviour can be done much more efficiently using hidden Markov models (HMMs) and associated frequentist inferential tools. HMMs are increasingly popular in this field, due to their flexibility and to the associated very efficient recursive algorithms available for conducting statistical inference  \citep{patterson2009, langrock2012}. The crucial requirements on movement data in order for HMMs to be suitable are that measurement error in positions is negligible and that there is a regular sampling unit (e.g.\ one positional observation per hour, or per dive, or any other meaningful unit).

\texttt{HMMoce} is an R package...% which implements HMMs and associated tools for state decoding, model selection etc.\ specifically tailored to animal movement modelling. Particular attention was paid to computational efficiency with the fitting algorithm implemented in C++. The high computational speed makes it feasible to analyze very large data sets --- e.g.\ tens of thousands of positions collected for each of a dozen individual animals --- on standard desktop PCs. The package also allows users to incorporate covariate data into their models, which is particularly useful when inferring the drivers of changes in behaviour.

Our hope is that the \texttt{moveHMM} package will provide users who collect movement data with an interface to sophisticated and adequate methods for a statistical analysis of their data. The package is structured so as to allow the users to prepare their data for analysis, fit a variety of HMMs to their data and perform diagnostics on these fitted models.

The package is presented in %\cite{braun2018a},
where its use is illustrated on example blue and mako shark PSAT data as well as compared to other commonly employed methods for geolocation of these and similar data. A subset of the data used in that paper is included as example data in the package.

%In this vignette, we briefly introduce HMMs in the context of animal movement. We then provide a detailed example of a typical use of the package (preprocessing movement data, fitting an HMM to the data, and analyzing the fitted model). Finally, we describe more technically the structure of the package and its main functions.

\section{Example application} \label{sec:application}

%Before we provide a detailed description of the various features of the \texttt{moveHMM} package in the subsequent section, we illustrate a typical HMM-based analysis of movement data using the main functions of the package, via an example. We use the data from \cite{morales2004}, collected on four elk in Canada.

Here we illustrate the use of the package with an example dataset from a blue shark tagged with a Wildlife Computers miniPAT in the NW Atlantic. Depending on which observation likelihoods (Sec. \ref{sec:lik}) you choose to generate for your species of interest, various tag-based data streams (e.g. light, SST) may be more or less useful. Our example dataset is from a blue shark which tends to occupy the epipelagic where the tag is able to consistently record high quality light and SST data that can be used for geolocation. Furthermore, this species tends to also make daily excursions, sometimes to $>$1000 m, which yields water column temperature profiles that can also be leveraged to improve geolocation estimates. Thus, it probably makes sense to take the time to build the 3D depth-temperature likelihoods in addition to the 2D, surface-only SST, for example. For other species or behaviors, such as benthic species that rarely interact with the sea surface, a bathymetry-based likelihood is probably more appropriate and perhaps SST is less informative.

\subsection{Loading and prepare the tag data} \label{sec:tag-data}

First, setup the start and end dates and locations. These are then used to build a vector of \texttt{POSIXct} timestamps that represent the desired time steps of the likelihoods and, ultimately, most probable track.
<<iniloc,size='small', tidy=TRUE>>=

# SET START/END LOCATIONS
## iniloc is dataframe containing cols: day, month, year, lat, lon and rows: start, end
iniloc <- data.frame(matrix(c(13, 10, 2015, 41.3, -69.27,
                              10, 4, 2016, 40.251, -36.061), nrow = 2, ncol = 5, byrow = T))
names(iniloc) <- list('day','month','year','lat','lon')
tag <- as.POSIXct(paste(iniloc[1,1], '/', iniloc[1,2], '/', iniloc[1,3], sep=''), format = '%d/%m/%Y', tz='UTC')
pop <- as.POSIXct(paste(iniloc[2,1], '/', iniloc[2,2], '/', iniloc[2,3], sep=''), format = '%d/%m/%Y', tz='UTC')

# VECTOR OF DATES FROM DATA. THIS WILL BE THE TIME STEPS, T, IN THE LIKELIHOODS
dateVec <- seq.POSIXt(tag, pop, by = '24 hours')
@

\subsubsection{Sea surface temperature data}

Next, we load some tag data. Raw example data is included with the package and can be read by pointing to the "extdata" directory associated with the install of the \texttt{HMMoce} package. Helper functions are provided for directly reading data from Wildlife Computers tags, however \texttt{HMMoce} works with any tag data that can be coerced to the minimum required input data for the various observation likelihoods. Over time, these have been generalized from WC-specific to more flexible implementations for use with other manufacturers. The minimum required SST data for use in \texttt{HMMoce}, and thus the output from the WC helper function, is "Date", "Depth", and "Temperature". Date must be of class POSIXct. Depth represents the depth at which the corresponding sea surface temperature ("Temperature") was taken. Currently the column "Depth" is not used but is retained here for user convenience in case, for example, it makes sense to filter the SST data due to SSTs being recorded deeper than what makes sense given the oceanography the tagged animal is in.

<<load_sst,size='small'>>=
sstFile <- system.file("extdata", "141259-SST.csv", package = "HMMoce")
tag.sst <- read.wc(sstFile, type = 'sst', tag=tag, pop=pop, verbose=F)
tag.sst <- tag.sst[,c('Date','Depth','Temperature')]
head(tag.sst)

@

\subsubsection{Depth-temperature data}

Next is some representation of water column structure. There are many options here based on tag model, animal behavior, etc. but ultimately the goal is to create a tag-based dataset that represents the thermal structure of the water column and contains columns at least for: "Date" (again POSIXct), "Depth", "MinTemp", and "MaxTemp". A fifth column called "MeanTemp" can be included which will be used as the temperature to for likelihood construction. If "MeanTemp" doesn't exist, \texttt{HMMoce} will by default calculate the midpoint between min and max temperature for each timepoint and use that.

By comparing tag-recorded thermal structure to the oceanographic environment, we can often add a very rich set of information for geolocation. For example, a tag sampling the thermal structure of the Sargasso Sea would yield very different results from that of the Gulf Stream or Labrador Sea. Tags like the example miniPAT used here provide summaries of depth-temperature profiles as a .csv called "-PDTs" for which there's a helper function.
<<load_pdt,size='small'>>=
# DEPTH-TEMPERATURE PROFILE DATA
## example is output from Wildlife Computer Portal
## pdt needs to contain at least:
##    - Date (POSIXct)
##    - Depth
##    - MinTemp
##    - MaxTemp
##    - MeanTemp (optional): if meantemp doesn't exist for whatever reason, HMMoce will calculate the midpoint between min/max temps and use that

pdtFile <- system.file("extdata", "141259-PDTs.csv", package = "HMMoce")
pdt <- read.wc(pdtFile, type = 'pdt', tag = tag, pop = pop, verbose = F)
pdt <- pdt[,c('Date','Depth','MinTemp','MaxTemp')]
head(pdt)
@

Depending on tag type, there are many ways to represent this vertical structure. For example, miniPATs can also report summaries of depth-temperature time series which could be used to re-construct custom depth-temperature profiles. Similarly, other models such as archival tags from Wildlife Computers or Lotek report a high-resolution time series of depth and temperature. These can be used to construct custom depth-temperature profiles. A few things to keep in mind for building custom summaries of this type of data:
\begin{itemize}
\item vertical depth levels are ultimately compared to whatever depth levels are available in the environmental dataset you compare to for building likelihoods. Thus, it might make sense to make your custom depth levels match, for example, the HYCOM depth levels you will compare the tag data to
\item temporal resolution of your summarized dataset can only be as high as the resolution of your `dateVec` object that is the "temporal backbone" of this entire modeling process. Thus, it makes sense that the timescales and resolution of these match, if possible.
\end{itemize}

<<load_series,size='small'>>=
# DEPTH-TEMPERATURE TIME SERIES DATA
## exampling showing how to coerce depth-temp time series to a PDT-like summarized product

## microwave or lotek, for example

@


\subsubsection{Light data} \label{sec:light-data}

Light data has been at the core of geolocation estimates in marine and terrestrial environments for decades (hill ref). As such, there are a number of methods for going from tag-based light measurements to likelihoods of where on Earth those light measurements were recorded. Taking light measurements from the back of a marine animal can be particularly error-prone as that animal is moving through a range of turbid to clear water, diving from the photic zone to great depths, and at least indirectly experiencing cloud cover and many other factors that can affect light measurements. Here, we focus on two main general types of light data one might get from an archival tag in the marine environment: raw light levels and light-based position estimates. This can be an important distinction as you'll see later when building likelihoods based on light (Sec. \ref{sec:lik}).

\vspace{2mm}
\noindent \textbf{"Raw" light data}

\noindent Raw light curves and/or sunrise and sunset times are recorded by most (if not all) archival tags. Our example miniPAT contains raw light curves but onboard processing has already determined sunrise and sunset times for us. If you go this route, required columns are "Date" (POSIXct) and "Type" where only "Dawn" and "Dusk" are recognized as valid types corresponding to sunrise and sunset, respectively.
<<light_raw,size='small', cache=F>>=

lightFile <- system.file("extdata", "141259-LightLoc.csv", package = "HMMoce")
light <- read.wc(lightFile, type = 'light', tag=tag, pop=pop, verbose=F)
## combine character vectors "Day" and "Time" to generate POSIXct object
light$Date <- lubridate::dmy_hms(paste(light$Day, light$Time, sep = ' '))
light <- light[,c('Date','Type')]
@

\noindent Our example miniPAT looks like this:

<<light_raw_ex,size='small'>>=
head(light)
@

\vspace{2mm}
\noindent \textbf{Light-based position estimates}

Many models of archival tag require manufacturer-specific post-processing which often generates light-based position estimates. These estimates often result in more realistic light-based likelihoods and are thus recommended over the "raw" light approach above. To incorporate light-based position estimates into the likelihood process, required columns are "Date" (POSIXct), estimated "Longitude", and "Error.Semi.minor.axis" which represents the error/uncertainty in the longitude estimate in METERS(!). With only these columns, you can generate longitude-only light-based likelihoods (i.e. if latitude estimates are unreliable and thus uninformative). To include latitude information and thus generate elliptical light-based likelihoods, the following additional columns are required: "Latitude", "Error.Semi.major.axis" (again, in meters), "Offset" (shift the ellipse this distance in METERS), "Offset.orientation" (currently 0 for North and 180 for South, no other orientation angles are currently supported). If your manufacturer didn't provide offset values, use 0 for both.
<<light_est,size='small'>>=

# LIGHT BASED POSITIONS FROM GPE2 (INSTEAD OF RAW LIGHTLOCS FROM PREVIOUS)
llFile <- system.file("extdata", "141259-Locations-GPE2.csv", package = "HMMoce")
lightloc <- read.table(llFile, sep = ',', header = T, blank.lines.skip = F)
lightloc <- lightloc[which(lightloc$Type != 'Argos'),]
lightloc <- lightloc[,c('Date','Longitude','Error.Semi.minor.axis','Latitude','Error.Semi.major.axis','Offset','Offset.orientation')]
lightloc$Date <- as.POSIXct(lightloc$Date, format = findDateFormat(lightloc$Date))
head(lightloc)
@


\subsubsection{Depth-only data} \label{sec:depth-data}

Most archival tags collect data on depth and many report statistics for depth, such as min and max, over a given summary period. It can often be useful to use at least the maximum depth over, for example, each day of a deployment to inform geolocation estimates. For benthic or bottom-oriented species, we can leverage both the max and min depths to constrain an animal's possible movements. To do this, we need some kind of summary of the depth data.

Our example miniPAT actually reports a summary sheet that already contains these values:

<<mmd,size='small'>>=
mmdFile <- system.file("extdata", "141259-MinMaxDepth.csv", package = "HMMoce")
mmd <- read.table(mmdFile, sep = ',', header = T, blank.lines.skip = F)[,c('Date','MinDepth','MaxDepth')]
mmd$Date <- as.POSIXct(mmd$Date, format = findDateFormat(mmd$Date))
head(mmd)

@

However, given a depth time series data stream from your tag, which is far more common, it is trivial to generate your own custom depth summary statistics. The benefit of the latter approach is full control over the temporal resolution of the summary if, for example, you wanted to generate likelihoods at finer timesteps than the onboard processing of depth summary statistics would allow.

<<mmd_series,size='small'>>=
seriesFile <- system.file("extdata", "141259-Series.csv", package = "HMMoce")
series <- read.table(seriesFile, sep = ',', header = T, blank.lines.skip = F)[,c('Day','Time','Depth','Temperature')]
series$Date <- as.Date(as.POSIXct(paste(series$Day, series$Time), format = '%d-%b-%Y %H:%M:%S', tz='UTC'))
mmd <- series %>% group_by(Date) %>% dplyr::summarise(n=n(), MinDepth = min(Depth, na.rm=T), MaxDepth = max(Depth, na.rm=T), .groups='keep')
head(mmd)

@

\subsubsection{Spatial limits}
The last data preparation step is to establish the spatial bounds of this model run. How you do this can vary widely based on species, their movement ecology, and the quality of the tag data used. This step is critical in the model setup because it \textbf{must} incorporate the complete geographic limits of your animal(s) movements! These limits are used for downloading environmental data and, ultimately, serve as the bounds for the model. Thus, err on the side of making the bounds too large to be sure the extent of movements are adequately captured.
If they aren't, the estimated movements will likely run into the edges of your spatial limits (specified here), and you will have to start over. The tradeoff as model bounds expand is, of course, computational demands increase as larger grids are computed. In the end, the choice of spatial bounds comes down to expert opinion so choose carefully.

This step is typically accomplished best by a combination of expert opinion (thats you!) and by looking at the spatial bounds of tagging and pop-up locations and longitude estimates (if you have them). If you don't have position estimates to start with, do some preliminary plotting of tag data (such as SST) to try to constrain where you think the animal went. There's no harm in having to come back to this when your model runs into the boundaries except that you'll have to download all the environmental data again which, you will soon find out, takes time. To that end, be smart about setting spatial limits when you plan to analyze data for a group of tag datasets. Find the largest common grid and use that for all analyses. That means you only have to download the oceanographic data once!

\noindent Here, we set our bounds over an extensive portion of the NW Atlantic as blue sharks are known to be highly migratory, and this individual could easily cover much of this area over the duration of the tag deployment. This decision was ultimately made by comparing the tag and pop-up locations to the depth-temperature information recorded onboard the tag to explore the likely water masses the animal encountered and thus the extent of movement.
<<splim,size='small'>>=
# SET SPATIAL LIMITS
# these are the lat/lon bounds of your study area (e.g. where you think the animal went)
sp.lim <- list(lonmin = -82,
               lonmax = -25,
               latmin = 15,
               latmax = 50)

sp.lim_small <- list(lonmin = -72,
                     lonmax = -66,
                     latmin = 38,
                     latmax = 43)

## setup the spatial grid to base likelihoods on
locs.grid <- setup.locs.grid(sp.lim, res='quarter')

@

\subsection{Environmental data} \label{sec:env}

Now that the tag data is prepared, the next major step is to acquire all the necessary environmental data to compare the tag data to. Depending on which likelihoods you choose to build, the required environmental data will vary. Light-based likelihoods require no environmental data.

If you plan to analyze multiple tag datasets over a similar time period and spatial domain, consider that before downloading the environmental datasets to save yourself from having to change spatial/temporal bounds later and download everything again. For a group of tags, combine the unique dates and movement extents across all tags and find a common spatial grid before downloading the dynamic environmental data (e.g. SST).

\subsubsection{SST data}

There are a number of SST products out there that meet our needs. While modeled SST products work fine, we typically use remote-sensing products. There are 3 products currently built into the \texttt{get.env} functionality within \texttt{HMMoce}. Here we'll use the Optimum Interpolation SST (OISST) from NOAA. Note the tradeoffs associated with different SST products such as gaps, gap-filling techniques/interpolations, and product resolution. Here we get one file as an example:

<<env-sst,size='small', cache=TRUE, eval=TRUE>>=
## a ridculously small grid for this species, just to show the examples
udates <- seq.Date(as.Date(tag), as.Date(pop), by = 'day')
sst.dir <- paste0(dir, '/EnvData/sst/')
if (!dir.exists(sst.dir)) dir.create(sst.dir, recursive = TRUE)
if (!file.exists(paste0(sst.dir, 'oisst_', udates[1], '.nc'))) get.env(udates[1], filename='oisst', type = 'sst', sst.type='oi', spatLim = sp.lim, save.dir = sst.dir)

sst <- raster::raster(paste0(sst.dir, 'oisst_', udates[1], '.nc'))
plot(sst, main='Example SST field')
world(add=T)
@

\subsubsection{Depth-temperature data}

As with SST above, there are a number of ways to represent temperature of the 3D ocean. While observations are nice, they are typically too sparse to generate a reliable grid on which we can calculated likelihoods. Thus, our only good option is to use oceanographic models. Data-assimilating models constantly "nudge" the model outputs to match reality and \texttt{HMMoce} includes functionality for accessing outputs of the HYbrid Coordinate Ocean Model (HYCOM). For most applications, models such as HYCOM are more likely to generate realistic likelihood results when comparing \textit{in situ} tag data to a dynamic ocean.

<<env-hycom,size='small', eval=TRUE, cache=TRUE>>=
## you need some representation of environmental depth-temperature
## in this case we're using hycom

hycom.dir <- paste0(dir,'/EnvData/hycom/')
if (!dir.exists(hycom.dir)) dir.create(hycom.dir, recursive = TRUE)
if (!file.exists(paste0(hycom.dir, 'hycom_', udates[1], '.nc'))) get.env(udates[1], filename='hycom', type = 'hycom', spatLim = sp.lim_small, save.dir = hycom.dir)

b <- raster::brick(paste0(hycom.dir, 'hycom_', udates[1], '.nc'))

## plot top 5 depth levels from HYCOM
plot(b[[c(1,10,20)]])
@

\subsubsection{Bathymetry}

The last "environmental" dataset we need is bathymetry. Bathymetry has multiple uses in \texttt{HMMoce} but is primarily used as a mask that eliminates portions of a likelihood grid that are shallower than the maximum depth the tag recorded during a given timestep. For primarily benthic species, bathymetry can also be used to generate a likelihood based on the tag-recorded depth compared to bottom depth.

<<env-bathy, size='small', eval=TRUE>>=

bathy.dir <- paste0(dir, '/EnvData/bathy/')
if (!dir.exists(bathy.dir)) dir.create(bathy.dir, recursive = TRUE)

if (!file.exists(paste0(bathy.dir, 'bathy.nc'))){
  bathy <- HMMoce::get.bath.data(sp.lim_small$lonmin, sp.lim_small$lonmax, sp.lim_small$latmin, sp.lim_small$latmax, folder = bathy.dir, res=1)
} else{ ## OR (once downloaded and reading the .nc later)
  bathy <- irregular_ncToRaster(paste0(bathy.dir, 'bathy.nc'), varid = 'topo')
}

## example bathymetry data
plot(bathy, main='Example bathymetry field')
world(add=T)
@


\subsection{Observation likelihoods} \label{sec:lik}

The basic premise of the observation likelihoods is that we compare a tag-based data stream to some representation of the environment to generate a likelihood surface. This comparison generates information about the likely location of the animal in the global ocean. Light levels and SST are the current paradigm for fish tracking and are the most straightforward to use for positioning. Three-dimensional depth-temperature data are more complex but provide rich information about oceanographic characteristics and water masses that animals experience as they move. Each data type has its own likelihood function(s) that does the grunt work for you. It often makes sense to build all the appropriate likelihoods given your tag data and study species and use model selection to determine the "best" combination of likelihoods \citep[for example, see][and section \ref{sec:multiple}]{Braun2018b}.

\subsubsection{Light likelihood}

Light-based geolocation is a complicated topic with a myriad of potential implementations for generating light-based likelihoods. For now, \texttt{HMMoce} has two main treatments of light to inform geolocation estimates (Sec. \ref{sec:light-data}). The simplest is using tag-based light levels to estimate sunrise and sunset times and thus position (\texttt{calc.srss}). This is often an overly simplistic treatment of this data so we often 1) throw out latitude estimates and only keep longitude and 2) get a lot of bad location information, particularly from species that don't frequent the photic zone. For example, a surface-oriented species that dives below the photic zone 30 minutes before sunset would generate an artificial sunset time 30 minutes early, causing $\sim$8$^\circ$ longitudinal difference in position estimate! The other approach currently implemented in \texttt{HMMoce} generates likelihoods based on existing light-based position estimates(\texttt{calc.lightloc}). Most manufacturers either provide the analysis or the analysis software to generate light-based position estimates. For example, Wildlife Computers uses a thresholding algorithm (hill ref) in their GPE2 software that allows user-controlled QC of light curves in a GUI. While this software has been replaced by their cloud-based GPE3 algorithm, GPE2 remains the tool of choice for this specific analysis as the user retains some control over the conversion of light curves to location estimates. Other manufacturers generate similar light-based position outputs such as Lotek's daily summary for archival tags. Additional light-based options will be implemented in the future as they become available and proven for marine animal telemetry.

<<lik-light, size='small', eval=FALSE>>=

  # LIGHT-BASED LIKELIHOODS
  L.light.srss <- calc.srss(light, locs.grid = locs.grid, dateVec = dateVec, res = 1, focalDim = 15)
  L.light <- calc.lightloc(lightloc, locs.grid = locs.grid, dateVec = dateVec, errEll = TRUE)

  ## other options?

@

Here's an example of what these look like:
<<lik-light-plot, size='small', eval=TRUE>>=

  load('./example_blueshark_lik_srss.rds')
  load('./example_blueshark_lik_lightloc_noEll.rds')
  load('./example_blueshark_lik_lightloc_wEll.rds')

  ## here's what the likelihoods look like
  par(mfrow=c(3,1))
  plot(L.light.srss[[12]], main='Sunrise/sunset based likelihood'); world(add=T)
  plot(L.light.noell[[12]], main='Light-based location likelihood - Lon-only'); world(add=T)
  plot(L.light.ell[[12]], main='Light-based location likelihood - Lon + Lat'); world(add=T)

@

\subsubsection{SST likelihood}

SST-based likelihoods are perhaps among the most simple, and often informative, for geolocation as SST can be highly dynamic and exhibit strong gradients over relatively small spatial scales. If your study animal regularly visits the surface and thus collects SST data, it's likely including that information will dramatically improve your results. In \texttt{HMMoce}, tag-based SST data is represented as a daily range in SST $\pm$ error (currently defaults to 1\%) and that SST envelope is then compared to a remotely-sensed SST product to generate a likelihood surface.

<<lik-sst, size='small', eval=FALSE>>=

  # SST LIKELIHOODS
  L.sst <- calc.sst(tag.sst, filename='oisst', sst.dir = sst.dir, dateVec = dateVec[1:5], sens.err = 1)
  L.sst <- calc.sst.par(tag.sst, filename='oisst', sst.dir = sst.dir, dateVec = dateVec, sens.err = 1, focalDim = 3, ncores=2)

@

Here's an example of what these look like:
<<lik-sst-plot, size='small', eval=TRUE>>=

  load('./example_blueshark_lik_sst.rds')

  ## here's what the likelihoods look like
  plot(L.sst[[12]], main='SST-based likelihood'); world(add=T)

@

\subsubsection{3D depth-temperature likelihood}

The depth-temperature based likelihoods allow users to generate 3D likelihoods for tagged animals to improve position estimates, which is particularly useful for study species that rarely visit the photic zone during the day \citep[(e.g. swordfish,][]{Neilson2009} or spend considerable periods of time in the mesopelagic \citep[e.g. basking sharks,][]{Skomal2009}. In \texttt{HMMoce}, there are currently two approaches to using the 3D data for geolocation. The first follows \citep{Luo2015} by integrating profile data to calculate Ocean Heat Content (OHC). We integrate tag-based depth-temperature data from a certain isotherm to the surface to calculate the "heat content" of that layer measured by the tagged animal. Similarly, we perform the same integration on the model ocean as represented in the HYbrid Coordinate Ocean Model ([HYCOM](http://hycom.org/)) and compare the two integrated metrics to generate a likelihood surface representing the animal's estimated daily position. The second approach is to use the profile in 3D space and compare it to oceanography at measured depth levels. This uses the same tag-based depth-temperature data (not integrated) and compares it to modeled oceanography (such as HYCOM). In either case, we use a linear regression to predict the tag-based temperature at the standard depth levels measured in the oceanographic datasets. Then a likelihood is calculated in the same fashion by comparing temperature from the tag to ocean temperature at each depth level and resulting likelihood layers are multiplied across depth levels to result in a single daily likelihood layer based on the tagged animal's dive data. Because these can be very computationally demanding, parallelized calculation functions are available for all three depth-temperature based likelihood functions (Sec. \ref{sec:comp}).

<<lik-3d, size='small', eval=FALSE>>=

  # OCEAN HEAT CONTENT (INTEGRATED PDTS)
  #L.ohc <- calc.ohc(pdt, filename='hycom', ohc.dir = hycom.dir, dateVec = dateVec, isotherm = '', use.se = F)
  L.ohc <- calc.ohc.par(pdt, filename='hycom', ohc.dir = hycom.dir, dateVec = dateVec, isotherm = '', use.se = F)

  # HYCOM PROFILE BASED LIKELIHOODS
  #L.hycom <- calc.hycom(pdt, filename='hycom', hycom.dir, focalDim = 9, dateVec = as.POSIXct(dateVec), use.se = T)
  L.hycom <- calc.hycom.par(pdt, filename='hycom', hycom.dir, focalDim = 9, dateVec = as.POSIXct(dateVec), use.se = T)
@

\subsubsection{Bathymetry likelihood}

Bathymetry can be used to further constrain the likely location of a tagged animal. The most common implementation of this is to use the maximum measured depth in each time step, compared to available bathymetry, to ensure the most probable track occupies water deep enough for the animal to make the observed vertical movements (in the implementation of \texttt{calc.bathy} this is referred to as \texttt{lik.type = 'max'}). The same approach could be used for a benthic species, but it may often make more sense for those species with reasonable certainty of interacting with the bottom to use the same approach as previous likelihoods and calculate a formal likelihood (\texttt{lik.type = 'dnorm'}). Since most bathymetry data is rather high resolution relative to the other environmental variables used here to calculate liklihoods, it usually will speed things up to down-sample the bathymetry before calculating the likelihoods, rather than waiting to resample prior to the actual modeling.

<<lik-bathy, size='small', eval=FALSE>>=

  ## bathymetry based likelihood
  ## resample bathy to a more reasonable (coarse) grid for likelihood calculations
  ## hi-res bathy grid will work but will take longer
  bathy_resamp <- raster::resample(bathy, L.sst) # or whatever grid makes sense to resample to
  L.bathy <- calc.bathy(mmd, bathy_resamp, dateVec, focalDim = 3, sens.err = 5, lik.type = 'dnorm')
  L.bathy <- calc.bathy.par(mmd, bathy_resamp, dateVec, focalDim = 3, sens.err = 5, lik.type = 'max', ncores = 4)

@


\subsubsection{Bottom temperature likelihood}

<<lik-bt, size='small', eval=FALSE>>=

  ## bottom temperature based likelihood
  L.bt <- calc.bottomTemp(tag.bt, dateVec, focalDim = 3, sens.err = 1, bt.dir = bt.dir, filename = 'bottomT', varName = 'Temperature')
  #L.bt <- calc.bottomTemp.par(tag.bt, dateVec, focalDim = 3, sens.err = 1, bt.dir = bt.dir, filename = 'bottomT', varName = 'Temperature', ncores = 4)
@


\subsubsection{Overall observation likelihood}

The final step in generating likelihoods is to combine the likelihoods of interest to generate an overall observation-based likelihood for each time step.
<<makeL, size='small', eval=FALSE>>=

# COMBINE LIKELIHOOD MATRICES
# L.idx combination indicates likelihood surfaces to consider
ras.list <- list(L.light, L.sst, L.ohc)

  #lik = L.res[[1]][L.idx[[tt]]],
L <- make.L(lik = ras.list,
            L.mle.res = L.res$L.mle.res, dateVec = dateVec,
            locs.grid = locs.grid, iniloc = iniloc, bathy = bathy,
            pdt = pdt)
@


\subsection{Computation}

AWS

all of the calc functions that have significant computational demands have a .par equivalent. just use the par arg for those funs

\subsection{Building models for multiple individuals} \label{sec:multiple}
pieces of an example script showing one approach to efficiently building multiple models for multiple individuals of the same study species that are expected to exhibit relatively similar mvoements and behaviors



\bibliographystyle{apalike}
\bibliography{hmm_ms}

\end{document}
